
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1.1. Convolution &#8212; Deep Learning for Experimental Psychologists and Cognitive Neuroscientists</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BXYWD71FWS"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BXYWD71FWS');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BXYWD71FWS');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/convolution';</script>
    <link rel="icon" href="../_static/icon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.2. Activation Functions" href="activation_function.html" />
    <link rel="prev" title="1. Network’s Building Blocks" href="networks_building_blocks.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Deep Learning for Experimental Psychologists and Cognitive Neuroscientists - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Deep Learning for Experimental Psychologists and Cognitive Neuroscientists - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Prerequisites</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../markdowns/environment_setup.html">0. Environment Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="networks_building_blocks.html">1. Network’s Building Blocks</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.1. Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="activation_function.html">1.2. Activation Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="pooling.html">1.3. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear.html">1.4. Linear Layer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">2. Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimisation_learning.html">3. Optimisation and Learning</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="vision.html">4. Vision</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="image_classification.html">4.1. Image Classification</a></li>





<li class="toctree-l2"><a class="reference internal" href="image_segmentation.html">4.2. Image Segmentation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="generative_models.html">5. Deep Generative Models</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="gan.html">5.1. Generative Adversarial Networks</a></li>






<li class="toctree-l2"><a class="reference internal" href="vae.html">5.2. Deep Autoencoders</a></li>







<li class="toctree-l2"><a class="reference internal" href="dpm.html">5.3. Diffusion Probabilistic Models</a></li>






</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="interpretation_techniques.html">6. Interpretation Techniques</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="activation.html">6.1. Activation Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesion.html">6.2. Kernel Lesioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_classifier_probe.html">6.3. Probing by linear classifiers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="big_projects.html">7. Big Projects</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../markdowns/python_scripting.html">7.1. Python Scripting</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorboard.html">7.2. TensorBoard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../markdowns/server.html">7.3. Working with Servers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="other_modalities.html">8. Other Modalities</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="audio_classification.html">8.1. Audio Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="text_classification.html">8.2. Text Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="clip.html">8.3. Language – Vision</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">9. Reinforcement Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="assignments/warmup.html">A1. Warming-up</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/dataloader.html">A2. Dataloaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/optimisation_learning.html">A3. Optimisation and Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="student_projects/deep-learning-with-dobble.html">Deep Learning with Dobble</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Python course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="python_course/beginners.html">Beginners</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="python_course/dataTypes.html">Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/modules.html">Modules and NumPy Arrays</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/conditions.html">Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/loops.html">Loops</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/plotting.html">Plotting</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/functions.html">Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/moduleObjects.html">Modules and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/inheritance.html">Inheritance</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/blob/master/notebooks/convolution.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/edit/master//notebooks/convolution.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/issues/new?title=Issue%20on%20page%20%2Fnotebooks/convolution.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/convolution.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>1.1. Convolution</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation">0. Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#required-packages">Required Packages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-image">Input Image</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualising-rgb-channels">Visualising RGB Channels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#opencv">1. OpenCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-detection">Edge Detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-blurring">Image Blurring</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">2. PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-tensors">Working with Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-detection-with-pytorch">Edge Detection with PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualise-output-tensors">Visualise Output Tensors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-size-in-convolutions">3. Kernel Size in Convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#times-1-convolution"><span class="math notranslate nohighlight">\(1 \times 1\)</span> Convolution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-in-convolution">4. Padding in Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#downsampling">5. Downsampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride">Stride</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dilation">6. Dilation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transposed-convolution">7. Transposed convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="convolution">
<h1>1.1. Convolution<a class="headerlink" href="#convolution" title="Link to this heading">#</a></h1>
<p>The remarkable success of modern deep learning models began with
<strong>Convolutional Neural Networks (CNNs)</strong>. In this notebook, we will introduce the fundamental
concept of the convolution operation, which is at the core of CNNs.</p>
<p>In mathematical terms, the <strong>discrete convolution</strong> of two functions, <span class="math notranslate nohighlight">\( f \)</span> and <span class="math notranslate nohighlight">\( g \)</span>, is expressed as:</p>
<p><span class="math notranslate nohighlight">\(
(f * g)[n] = \sum_{m=-M}^{M} f[n-m] g[m]
\)</span></p>
<p>Here, <span class="math notranslate nohighlight">\( M \)</span> represents the size of the window that slides across the input, applying the operation at each step. This sliding action is why convolution is often referred to as a <strong>sliding window</strong> operation. Simply put, convolution involves calculating a weighted sum of neighbouring elements within a signal or image.</p>
<p>Convolution is a widely-used technique in <strong>signal processing</strong> and <strong>image analysis</strong>. For instance, classical image processing algorithms often rely on this operation to filter or transform images. Given its importance, convolution is implemented in many popular programming libraries, making it easier to apply in practice.</p>
<a href="https://en.wikipedia.org/wiki/Convolution#Discrete_convolution" target="_blank">
    <img src="https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif" />
</a>
<p>To visualise the process, think of convolution as involving three matrices:</p>
<ol class="arabic simple">
<li><p><strong>Input</strong>: The original data or signal (e.g., an image).</p></li>
<li><p><strong>Kernel (Filter)</strong>: A smaller matrix, defining the neighbouring weights, that is slid over the input.</p></li>
<li><p><strong>Output</strong>: The resulting matrix after applying the kernel to the input.</p></li>
</ol>
<p>As the kernel moves across the input, at each position, it computes a weighted sum of the values under it, which is then placed in the corresponding position of the output matrix. This process repeats until every element of the input has been covered.</p>
<p>In this notebook, we will explore different ways of implementing the convolution operation using common libraries. By the end of this section, you will have a hands-on understanding of how convolution works, along with the practical details required to apply it in various contexts.</p>
<section id="preparation">
<h2>0. Preparation<a class="headerlink" href="#preparation" title="Link to this heading">#</a></h2>
<p>Before we dive into the main content of this notebook, let’s start by preparing all the necessary materials.</p>
<section id="required-packages">
<h3>Required Packages<a class="headerlink" href="#required-packages" title="Link to this heading">#</a></h3>
<p>We will need several libraries to implement this tutorial, each serving a specific purpose:</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://numpy.org/">numpy</a>:</strong> The main package for scientific computing in Python. It’s commonly imported with the alias <code class="docutils literal notranslate"><span class="pre">np</span></code> to simplify its use in code.</p></li>
<li><p><strong><a class="reference external" href="https://matplotlib.org/">matplotlib</a>:</strong> A library for plotting graphs and visualising data in Python.</p></li>
<li><p><strong><a class="reference external" href="https://pytorch.org/docs/stable/index.html">torch</a>:</strong> A deep learning framework that helps us define neural networks, manage datasets, optimise loss functions, and more.</p></li>
<li><p><strong><a class="reference external" href="https://docs.opencv.org/4.x/index.html">cv2</a> (OpenCV):</strong> A widely-used computer vision library for image processing and computer vision tasks.</p></li>
<li><p><strong><a class="reference external" href="https://scikit-image.org/">skimage</a>:</strong> A collection of algorithms for image processing, ideal for various transformation and analysis tasks.</p></li>
</ul>
<p>Now, let’s go ahead and import these packages:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing the necessary packages/libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  <span class="c1"># The &quot;as&quot; keyword allows us to give the package a shorthand name</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>  <span class="c1"># For plotting graphs</span>
<span class="kn">import</span> <span class="nn">cv2</span>  <span class="c1"># OpenCV for computer vision tasks</span>
<span class="kn">import</span> <span class="nn">skimage</span>  <span class="c1"># For image processing</span>
<span class="kn">import</span> <span class="nn">torch</span>  <span class="c1"># PyTorch for deep learning</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="input-image">
<h3>Input Image<a class="headerlink" href="#input-image" title="Link to this heading">#</a></h3>
<p>In this section, we will work with images to observe the effect of convolution on them. We begin by loading two images from URLs using the <code class="docutils literal notranslate"><span class="pre">skimage.io.imread</span></code> function. After loading the images, we will visualise them using routines from the <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># URLs of the images we will load</span>
<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/images/plain/normal/color/295087.jpg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/images/plain/normal/color/35008.jpg&#39;</span>
<span class="p">]</span>

<span class="c1"># Load the images using list comprehension</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">skimage</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>

<span class="c1"># Visualise the two images side by side</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create a figure with a custom size</span>
<span class="k">for</span> <span class="n">img_ind</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">img_ind</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Add subplots for each image</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># Display the image</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a702dd60629edfd8559d359ddd1ce49107b43f98c296e9b2c32d4a1645e25dab.png" src="../_images/a702dd60629edfd8559d359ddd1ce49107b43f98c296e9b2c32d4a1645e25dab.png" />
</div>
</div>
<p>Next, let’s check the <strong>shape</strong> and <strong>data type</strong> of the images to better understand their structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the size (shape) and data type of the first image</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image size:&quot;</span><span class="p">,</span> <span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image type:&quot;</span><span class="p">,</span> <span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image size: (321, 481, 3)
Image type: uint8
</pre></div>
</div>
</div>
</div>
<p>In most cases:</p>
<ul class="simple">
<li><p><strong>RGB images</strong> have three dimensions: <code class="docutils literal notranslate"><span class="pre">(height,</span> <span class="pre">width,</span> <span class="pre">channels=3)</span></code>, where the third dimension represents the Red, Green, and Blue (RGB) channels.</p></li>
<li><p><strong>Greyscale images</strong> only have two dimensions: <code class="docutils literal notranslate"><span class="pre">(height,</span> <span class="pre">width)</span></code>.</p></li>
</ul>
<p>In this example, the images have a height of <span class="math notranslate nohighlight">\(h = 321\)</span> and a width of <span class="math notranslate nohighlight">\(w = 481\)</span>.</p>
<p>Most images, including the ones we are working with, are stored in the <strong>unsigned integer format</strong> <code class="docutils literal notranslate"><span class="pre">uint8</span></code>, meaning pixel values range from 0 to 255</p>
</section>
<section id="visualising-rgb-channels">
<h3>Visualising RGB Channels<a class="headerlink" href="#visualising-rgb-channels" title="Link to this heading">#</a></h3>
<p>To gain a better understanding of how images are represented, let’s plot each individual RGB channel (Red, Green, and Blue) separately. This will allow us to see the contribution of each colour channel to the overall image.</p>
<p>We will define a function to visualise the three channels of an image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Function to plot the three colour channels separately</span>
<span class="k">def</span> <span class="nf">plot_3channels</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">channel_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Red&#39;</span><span class="p">,</span> <span class="s1">&#39;Green&#39;</span><span class="p">,</span> <span class="s1">&#39;Blue&#39;</span><span class="p">]</span>  <span class="c1"># Labels for each channel</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create a wide figure</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span>  <span class="c1"># Loop through the three channels</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Create three subplots</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>  <span class="c1"># Display each channel as a greyscale image</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>  <span class="c1"># Turn off axes for clarity</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">channel_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># Set the title to the channel&#39;s name</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># showing the first image</span>
<span class="n">plot_3channels</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9dde11ed457e64f86c35cd8123cddfeeefa0698023707c29f8d1711fde5a8bb7.png" src="../_images/9dde11ed457e64f86c35cd8123cddfeeefa0698023707c29f8d1711fde5a8bb7.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># showing the second image</span>
<span class="n">plot_3channels</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/56107c05bcf51a1afd174508722f573b278b8f3b5e3e5307d5463d3a1d50a6d3.png" src="../_images/56107c05bcf51a1afd174508722f573b278b8f3b5e3e5307d5463d3a1d50a6d3.png" />
</div>
</div>
</section>
</section>
<section id="opencv">
<h2>1. OpenCV<a class="headerlink" href="#opencv" title="Link to this heading">#</a></h2>
<p>Now that we have loaded our input images, we will use the <strong>OpenCV</strong> library (<code class="docutils literal notranslate"><span class="pre">cv2</span></code>) to explore the convolution operation. In particular, we will use the <code class="docutils literal notranslate"><span class="pre">filter2D</span></code> function, which performs convolution, to investigate two key applications of convolution in image processing: <strong>edge detection</strong> and <strong>image blurring</strong>.</p>
<section id="edge-detection">
<h3>Edge Detection<a class="headerlink" href="#edge-detection" title="Link to this heading">#</a></h3>
<p>First, we will design a simple <strong>edge detection kernel</strong> that is sensitive to vertical edges. A kernel (or filter) is typically a small matrix used to detect certain features in an image, such as edges or textures. These small matrices make convolution computationally efficient and widely used in image processing.</p>
<p>In this example, we define a <strong>3x3 edge detection kernel</strong>. The matrix has opposite (antagonistic) signs in the first and third columns, making it sensitive to vertical edges in the image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining a simple 3x3 edge detection kernel</span>
<span class="n">edge_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Detects vertical changes in pixel intensity</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Rows are identical, looking for vertical edges</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Printing the kernel to understand its structure</span>
<span class="nb">print</span><span class="p">(</span><span class="n">edge_kernel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1  0 -1]
 [ 1  0 -1]
 [ 1  0 -1]]
</pre></div>
</div>
</div>
</div>
<p>Here:</p>
<ul class="simple">
<li><p>Positive values (<code class="docutils literal notranslate"><span class="pre">1</span></code>) on the left and negative values (<code class="docutils literal notranslate"><span class="pre">-1</span></code>) on the right help detect changes in pixel intensity from left to right.</p></li>
<li><p>When this kernel slides over the image, it highlights vertical edges by calculating the difference in intensity between neighbouring pixels.</p></li>
</ul>
<p>Next, we apply the <code class="docutils literal notranslate"><span class="pre">cv2.filter2D</span></code> function, which performs convolution, to each image using the edge detection kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Applying the edge detection kernel to each image using OpenCV&#39;s filter2D function</span>
<span class="n">edge_imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cv2</span><span class="o">.</span><span class="n">filter2D</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">ddepth</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">edge_kernel</span><span class="p">)</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">imgs</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>In this code:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">filter2D()</span></code></strong>: This function convolves the image with the given kernel. The parameter <code class="docutils literal notranslate"><span class="pre">ddepth=-1</span></code> ensures the output image has the same depth (or data type) as the input image.</p></li>
<li><p>We apply the edge detection filter to both images and store the results in <code class="docutils literal notranslate"><span class="pre">edge_imgs</span></code>.</p></li>
</ul>
<p>Let’s now visualise the edges detected in both images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualising the edge-detected images</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create a figure for displaying images</span>
<span class="k">for</span> <span class="n">img_ind</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">edge_imgs</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">img_ind</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Add a subplot for each image</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># Display the edge-detected image</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3b2147b2c44e61884eaa326ce72e4cea8f7be35718015c21d367c8ab2d394a1b.png" src="../_images/3b2147b2c44e61884eaa326ce72e4cea8f7be35718015c21d367c8ab2d394a1b.png" />
</div>
</div>
<p>The output may look distorted because the edges in the RGB colour channels may not align perfectly. To better understand the results, we can display the detected edges for each colour channel separately (Red, Green, and Blue).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the edges in the first image</span>
<span class="n">plot_3channels</span><span class="p">(</span><span class="n">edge_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a2492c4ded239bbc67429ee60ec9bb68bd25bbbe0075e7a69d65a4f9fabb47b0.png" src="../_images/a2492c4ded239bbc67429ee60ec9bb68bd25bbbe0075e7a69d65a4f9fabb47b0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the edges in the second image</span>
<span class="n">plot_3channels</span><span class="p">(</span><span class="n">edge_imgs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c8c4871f680cafc0769b04ffccec5df852c76ed6558d557297ca67d029a4cbf6.png" src="../_images/c8c4871f680cafc0769b04ffccec5df852c76ed6558d557297ca67d029a4cbf6.png" />
</div>
</div>
<p><strong>Question:</strong> What happens if we increase the size of the kernel (e.g., from 3x3 to 5x5 or larger)?</p>
<p>You can experiment with different kernel sizes by modifying the <code class="docutils literal notranslate"><span class="pre">edge_kernel</span></code> matrix. Larger kernels will affect a larger neighbourhood of pixels, which might result in different edges.</p>
</section>
<section id="image-blurring">
<h3>Image Blurring<a class="headerlink" href="#image-blurring" title="Link to this heading">#</a></h3>
<p>To blur an image, we can use a convolution operation with an “averaging” kernel. This kernel smooths the image by averaging the pixel values within a sliding window, reducing sharp edges and fine details.</p>
<p>We will start by creating a <strong>5x5 averaging kernel</strong>. Each element in this kernel has an equal weight, meaning that the convolution will compute the mean value of the pixel intensities within the window it slides over.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the size of the kernel (5x5)</span>
<span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Create the averaging kernel with equal weights (normalised by the total number of elements)</span>
<span class="n">avg_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Print the kernel to understand its structure</span>
<span class="nb">print</span><span class="p">(</span><span class="n">avg_kernel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.04 0.04 0.04 0.04 0.04]
 [0.04 0.04 0.04 0.04 0.04]
 [0.04 0.04 0.04 0.04 0.04]
 [0.04 0.04 0.04 0.04 0.04]
 [0.04 0.04 0.04 0.04 0.04]]
</pre></div>
</div>
</div>
</div>
<p>In the code above:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">np.ones((kernel_size,</span> <span class="pre">kernel_size))</span></code></strong> creates a matrix where all values are <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>We then divide each element by <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">**</span> <span class="pre">2</span></code> (in this case, 25) to normalise the values, ensuring the sum of the kernel elements is <code class="docutils literal notranslate"><span class="pre">1</span></code>. This ensures that the average of the pixel values in the window is computed.</p></li>
</ul>
<p>Now, we will apply this blurring kernel to our input images using the same <code class="docutils literal notranslate"><span class="pre">cv2.filter2D</span></code> function as before. This will smooth out details and make the images appear less sharp.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply the blurring kernel to each image using OpenCV&#39;s filter2D function</span>
<span class="n">blurred_imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cv2</span><span class="o">.</span><span class="n">filter2D</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">ddepth</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">avg_kernel</span><span class="p">)</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">imgs</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>As with edge detection, <strong><code class="docutils literal notranslate"><span class="pre">filter2D()</span></code></strong> performs convolution. Here, the kernel computes the average of the pixel values in a 5x5 neighbourhood for each pixel, producing a blurred version of the image.</p></li>
</ul>
<p>Let’s visualise the results to observe how the images have been blurred.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualising the blurred images</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create a figure to display the images</span>
<span class="k">for</span> <span class="n">img_ind</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">blurred_imgs</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">img_ind</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Add subplots for each image</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># Display the blurred image</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4836b876211a01a2edbed8f1bfc1b0fe0f0b4185f60b2817050ee399213497b2.png" src="../_images/4836b876211a01a2edbed8f1bfc1b0fe0f0b4185f60b2817050ee399213497b2.png" />
</div>
</div>
<p>In this visualisation:</p>
<ul class="simple">
<li><p>The images will appear smoother and less sharp compared to the original versions, as the averaging kernel reduces the intensity of edges and fine details.</p></li>
</ul>
<p>These two simple examples—<strong>edge detection</strong> and <strong>image blurring</strong>—demonstrate the power of the convolution operation. Despite being a basic operation, convolution is foundational in many image processing tasks. CNNs leverage this operation by combining thousands of convolutional layers to detect complex patterns in data, ultimately forming powerful models for tasks such as image recognition and classification.</p>
<p>In the next section, we will explore how convolution operations are implemented in the <strong>PyTorch</strong> library, which we will be using throughout this course for deep learning applications.</p>
</section>
</section>
<section id="pytorch">
<h2>2. PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h2>
<p>In this section, we will explore how to perform similar convolution operations using <strong>PyTorch</strong>. While PyTorch functions are designed to be used within neural networks, here we will use them on single images to help you better understand the fundamental operations. In this tutorial, we will focus on <strong>edge detection</strong>, but you can try image blurring on your own as an exercise.</p>
<section id="working-with-tensors">
<h3>Working with Tensors<a class="headerlink" href="#working-with-tensors" title="Link to this heading">#</a></h3>
<p>PyTorch uses a different format for images compared to libraries like OpenCV. Specifically:</p>
<ul class="simple">
<li><p>The image type should be <code class="docutils literal notranslate"><span class="pre">float</span></code> (not <code class="docutils literal notranslate"><span class="pre">uint8</span></code>).</p></li>
<li><p>The <strong>channel dimension</strong> (representing colour channels) must come before the <strong>spatial dimensions</strong> (width and height), i.e., the shape should be <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">h,</span> <span class="pre">w)</span></code> for an RGB image.</p></li>
<li><p>PyTorch functions typically expect 4D tensors, where the first dimension corresponds to a batch of images. In our case, since we have two images, the shape will be <code class="docutils literal notranslate"><span class="pre">(b,</span> <span class="pre">3,</span> <span class="pre">h,</span> <span class="pre">w)</span></code> with <code class="docutils literal notranslate"><span class="pre">b=2</span></code>.</p></li>
</ul>
<p>Let’s begin by converting our images to the format expected by PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the images from uint8 (0-255) to float32 and scale to [0, 1]</span>
<span class="n">torch_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span> <span class="o">/</span> <span class="mi">255</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">imgs</span><span class="p">]</span>

<span class="c1"># Permute the dimensions to move the colour channels (RGB) first</span>
<span class="n">torch_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">torch_tensor</span> <span class="ow">in</span> <span class="n">torch_tensors</span><span class="p">]</span>

<span class="c1"># Stack both images into one 4D tensor (batch_size, channels, height, width)</span>
<span class="n">torch_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Check the size of the resulting tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tensor size:&quot;</span><span class="p">,</span> <span class="n">torch_tensors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor size: torch.Size([2, 3, 321, 481])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code></strong> converts the NumPy array to a PyTorch tensor.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.permute()</span></code></strong> rearranges the dimensions to put the colour channels first.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.stack()</span></code></strong> combines both images into a single tensor with a batch dimension.</p></li>
</ul>
</section>
<section id="edge-detection-with-pytorch">
<h3>Edge Detection with PyTorch<a class="headerlink" href="#edge-detection-with-pytorch" title="Link to this heading">#</a></h3>
<p>We will now apply a convolution operation in PyTorch using the <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> function. This function will convolve the input tensor with a kernel, similar to what we did in OpenCV.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Conv2d layer with 3 input channels (RGB) and 1 output channel (for edges)</span>
<span class="n">torch_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Disable gradient computation since we&#39;re not training a model</span>
<span class="n">torch_conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Print the initial random weights of the kernel</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[[[-0.1393,  0.1269,  0.1215],
          [ 0.0989, -0.1230, -0.1475],
          [ 0.0788, -0.1871, -0.1472]],

         [[ 0.0840, -0.1888, -0.0002],
          [ 0.1656, -0.0563, -0.1842],
          [ 0.1522, -0.0847,  0.0265]],

         [[ 0.1698,  0.1811, -0.1898],
          [ 0.1823,  0.0958, -0.0037],
          [-0.1371,  0.1908, -0.0690]]]])
torch.Size([1, 3, 3, 3])
</pre></div>
</div>
</div>
</div>
<p>In the code above:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code></strong> creates a convolution layer. We specify:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">in_channels=3</span></code>: The number of input channels (RGB).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_channels=1</span></code>: We want to produce a single channel (edge detection).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size=(3,</span> <span class="pre">3)</span></code>: The size of the kernel.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code></strong> ensures the weights won’t be updated, as we’re not training a network.</p></li>
</ul>
<p>The initial weights of the kernel we created above are random. We will replace them with the <strong>edge detection kernel</strong> we used earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fill the convolution kernel with the predefined edge detection kernel</span>
<span class="n">torch_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Print the updated kernel to verify the change</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[[[ 1.,  0., -1.],
          [ 1.,  0., -1.],
          [ 1.,  0., -1.]],

         [[ 1.,  0., -1.],
          [ 1.,  0., -1.],
          [ 1.,  0., -1.]],

         [[ 1.,  0., -1.],
          [ 1.,  0., -1.],
          [ 1.,  0., -1.]]]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code></strong> converts the edge kernel to a PyTorch tensor.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">np.stack()</span></code></strong> replicates the edge kernel across all three colour channels (R, G, B).</p></li>
</ul>
<p>Now, we can apply the convolution operation to detect edges in the images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply the convolution function to the tensor containing the images</span>
<span class="n">torch_edges</span> <span class="o">=</span> <span class="n">torch_conv</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_edges</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 319, 479])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The output tensor will have the shape <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">1,</span> <span class="pre">W,</span> <span class="pre">H)</span></code>, where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">2</span></code> corresponds to the two input images.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1</span></code> is the number of output channels (edges).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">H</span></code> are the width and height of the resulting images.</p></li>
</ul>
</li>
</ul>
<p><strong>Question</strong>: Why is the output’s width and height smaller than the input’s original dimensions (<span class="math notranslate nohighlight">\(321 \times 481\)</span>)?</p>
</section>
<section id="visualise-output-tensors">
<h3>Visualise Output Tensors<a class="headerlink" href="#visualise-output-tensors" title="Link to this heading">#</a></h3>
<p>To visualise the results, we will convert the PyTorch tensor back to NumPy and display the images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the tensor back to NumPy for visualisation</span>
<span class="n">torch_edges_np</span> <span class="o">=</span> <span class="n">torch_edges</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># detach from the computational graph</span>
<span class="n">torch_edges_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">torch_edges_np</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Rearrange dimensions for plotting</span>

<span class="c1"># Visualising the edge-detected images</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">img_ind</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">torch_edges_np</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">img_ind</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>  <span class="c1"># Display the edges in grayscale</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3ea1a6e56ede44531eb4a08fec2d56c445383215d24896baf9f5af50ca68f415.png" src="../_images/3ea1a6e56ede44531eb4a08fec2d56c445383215d24896baf9f5af50ca68f415.png" />
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.detach()</span></code></strong> detaches the tensor from the computation graph, allowing us to convert it to NumPy without gradients.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">np.transpose()</span></code></strong> reorders the dimensions to make it compatible for plotting with <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>.</p></li>
</ul>
<p><strong>Question</strong>: Why do the results look different from what we obtained using OpenCV?</p>
</section>
</section>
<section id="kernel-size-in-convolutions">
<h2>3. Kernel Size in Convolutions<a class="headerlink" href="#kernel-size-in-convolutions" title="Link to this heading">#</a></h2>
<p>Convolutional kernels typically have an odd-sized window. This makes it easier to centre the kernel over each pixel in the input image during the convolution process. The size of the kernel can be specified for each dimension independently by passing a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> to the <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> argument.</p>
<p>For example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size=(3,</span> <span class="pre">3)</span></code> defines a square window of 3 rows and 3 columns.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size=(11,</span> <span class="pre">3)</span></code> defines a rectangular window spanning 11 rows and 3 columns.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size=(5,</span> <span class="pre">1)</span></code> applies convolution only along the rows.</p></li>
</ul>
<p>As the kernel size increases, the computational cost also increases. This is because the number of computations needed for the convolution grows <strong>quadratically</strong> with the kernel size. For instance, if we increase the kernel size from <code class="docutils literal notranslate"><span class="pre">3x3</span></code> to <code class="docutils literal notranslate"><span class="pre">5x5</span></code>, the number of calculations increases by a factor of 2.25 (since <span class="math notranslate nohighlight">\(5^2 / 3^2 = 2.25\)</span>). Critically, this increase in computational time occurs at every pixel. This can significantly impact both the time and memory required to perform the operation, especially on large images.</p>
<section id="times-1-convolution">
<h3><span class="math notranslate nohighlight">\(1 \times 1\)</span> Convolution<a class="headerlink" href="#times-1-convolution" title="Link to this heading">#</a></h3>
<p>A <strong><span class="math notranslate nohighlight">\(1 \times 1\)</span> convolution</strong> does not perform any spatial convolution. Instead, it only convolves across the depth (i.e., the colour channels). This is useful for increasing or decreasing the number of channels without changing the image’s spatial resolution. It’s commonly used in deep learning to adjust the number of feature maps.</p>
</section>
</section>
<section id="padding-in-convolution">
<h2>4. Padding in Convolution<a class="headerlink" href="#padding-in-convolution" title="Link to this heading">#</a></h2>
<p>When applying convolution, an issue arises at the <strong>border pixels</strong> because part of the convolution kernel may fall outside the input image. To handle this, we use <strong>padding</strong>, which adds extra pixels around the edges of the image. There are several types of padding:</p>
<ul class="simple">
<li><p><strong>‘constant’</strong>: Pads with a constant value (e.g., zeros).</p></li>
<li><p><strong>‘reflect’</strong>: Reflects the edge of the image.</p></li>
<li><p><strong>‘replicate’</strong>: Replicates the border pixel values.</p></li>
<li><p><strong>‘circular’</strong>: Wraps the image in a circular manner.</p></li>
</ul>
<p>If we want the output image to have the same size as the input, we need to add padding of size <span class="math notranslate nohighlight">\(\frac{kernel\_size}{2}\)</span> (rounded down). For example, for a <code class="docutils literal notranslate"><span class="pre">3x3</span></code> kernel, we add <code class="docutils literal notranslate"><span class="pre">padding=1</span></code> to ensure the output dimensions match the input dimensions.</p>
<p>Here’s an example of applying convolution with padding:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Conv2D layer with padding to maintain the same output size as the input</span>
<span class="n">torch_conv_pad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Disable gradient computation since we&#39;re not training a model</span>
<span class="n">torch_conv_pad</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fill the convolution kernel with the predefined edge detection kernel</span>
<span class="n">torch_conv_pad</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply the convolution to the image tensor</span>
<span class="n">torch_edges_pad</span> <span class="o">=</span> <span class="n">torch_conv_pad</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_edges_pad</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 321, 481])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">padding=1</span></code></strong> adds one layer of padding around the image, allowing the convolution to include the border pixels.</p></li>
</ul>
</section>
<section id="downsampling">
<h2>5. Downsampling<a class="headerlink" href="#downsampling" title="Link to this heading">#</a></h2>
<p>One common use of convolution is to <strong>downsample</strong> the input image, reducing its spatial resolution. One way to achieve this is by <strong>not using padding</strong>. When no padding is applied, the output size is reduced by <span class="math notranslate nohighlight">\(\frac{kernel\_size}{2} + 1\)</span> because the convolution cannot process the border pixels.</p>
<section id="stride">
<h3>Stride<a class="headerlink" href="#stride" title="Link to this heading">#</a></h3>
<p>Another way to downsample the input is by using the <strong>stride</strong> parameter in the convolution operation. <strong>Stride</strong> refers to the number of pixels skipped during the convolution process. For instance, if <code class="docutils literal notranslate"><span class="pre">stride=2</span></code>, the convolution jumps every other pixel, effectively downsampling the image by a factor of two.</p>
<p>The stride can also be set independently for each dimension, much like the kernel size. For example:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">stride=2</span></code></strong> will downsample both dimensions by a factor of two.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">stride=(2,</span> <span class="pre">1)</span></code></strong> will downsample only the rows, while the columns remain unaffected.</p></li>
</ul>
<p>Here’s an example of applying convolution with <code class="docutils literal notranslate"><span class="pre">stride=2</span></code> to downsample the image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply convolution with stride=2 to downsample both dimensions</span>
<span class="n">torch_conv_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># Disable gradient computation since we&#39;re not training a model</span>
<span class="n">torch_conv_stride</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fill the convolution kernel with the predefined edge detection kernel</span>
<span class="n">torch_conv_stride</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply the convolution to the image tensor</span>
<span class="n">out_conv_stride</span> <span class="o">=</span> <span class="n">torch_conv_stride</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out_conv_stride</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 161, 241])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In this case, the output will have half the resolution of the input in both the width and height due to <code class="docutils literal notranslate"><span class="pre">stride=2</span></code>.</p></li>
</ul>
<p>Let’s see how we can downsample <strong>only the rows</strong> by setting <code class="docutils literal notranslate"><span class="pre">stride=(2,</span> <span class="pre">1)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply convolution with stride=(2, 1) to downsample only the rows</span>
<span class="n">torch_conv_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Disable gradient computation since we&#39;re not training a model</span>
<span class="n">torch_conv_stride</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fill the convolution kernel with the predefined edge detection kernel</span>
<span class="n">torch_conv_stride</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply the convolution to the image tensor</span>
<span class="n">out_conv_stride</span> <span class="o">=</span> <span class="n">torch_conv_stride</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out_conv_stride</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 161, 481])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Here, <code class="docutils literal notranslate"><span class="pre">stride=(2,</span> <span class="pre">1)</span></code> means we downsample only the rows by a factor of two, while the columns remain at the same resolution.</p></li>
</ul>
</section>
</section>
<section id="dilation">
<h2>6. Dilation<a class="headerlink" href="#dilation" title="Link to this heading">#</a></h2>
<p>A useful way to expand the <strong>receptive field</strong> of a convolutional layer without significantly increasing computational cost is by using the <code class="docutils literal notranslate"><span class="pre">dilation</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> function. Known as the <strong>à trous algorithm</strong> (meaning “with holes”), <code class="docutils literal notranslate"><span class="pre">dilation</span></code> adjusts the spacing between kernel elements, allowing the convolutional layer to cover a larger area of the input image. This is different from <code class="docutils literal notranslate"><span class="pre">stride</span></code>, which controls the spacing between the points where the convolution is applied.</p>
<p>Below is an illustration showing how dilation works by adding gaps between elements in the kernel.</p>
<img src="https://raw.githubusercontent.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/refs/heads/master/imgs/convolution_dilation.png" />
<p>Let’s use <code class="docutils literal notranslate"><span class="pre">dilation=3</span></code> for edge detection to see how this influences the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Conv2D layer with a dilation parameter to expand the receptive field</span>
<span class="n">torch_conv_dilation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>

<span class="c1"># Disable gradient computation as we’re not training a model</span>
<span class="n">torch_conv_dilation</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fill the kernel with a predefined edge detection kernel</span>
<span class="n">torch_conv_dilation</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply the convolution to the image tensor</span>
<span class="n">out_conv_dilation</span> <span class="o">=</span> <span class="n">torch_conv_dilation</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out_conv_dilation</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 321, 481])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Notice that <code class="docutils literal notranslate"><span class="pre">padding=3</span></code> is used here to ensure the output image has the same spatial resolution as the input.</p></li>
</ul>
<p>Let’s visualise the edges detected with the dilated kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the tensor back to a NumPy array for visualisation</span>
<span class="n">out_conv_np</span> <span class="o">=</span> <span class="n">out_conv_dilation</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Detach from computational graph</span>
<span class="n">out_conv_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">out_conv_np</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Rearrange for plotting</span>

<span class="c1"># Displaying the images with detected edges</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">img_ind</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">out_conv_np</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">img_ind</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>  <span class="c1"># Show edges in grayscale</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5f12b20efba186382841ecc341dfd09a442a3ba6eb614b7d56a5db1db00d288d.png" src="../_images/5f12b20efba186382841ecc341dfd09a442a3ba6eb614b7d56a5db1db00d288d.png" />
</div>
</div>
<p><strong>Question</strong>: Compare this output with the result when <code class="docutils literal notranslate"><span class="pre">dilation=1</span></code>. How has dilation affected the output edges?</p>
</section>
<section id="transposed-convolution">
<h2>7. Transposed convolution<a class="headerlink" href="#transposed-convolution" title="Link to this heading">#</a></h2>
<p>The convolutional layers we have discussed so far typically reduce the spatial dimensions of the input, a process known as <strong>downsampling</strong>. In certain tasks, however, we may want to <strong>increase the spatial resolution</strong>. For example, in <strong>semantic segmentation</strong>, the model’s output often needs to match the spatial resolution of the input image. As convolutional layers typically downsample the input, <strong>transposed convolution</strong> offers a method to increase the resolution of feature maps in the network.</p>
<p>Though it is sometimes referred to as “deconvolution”, transposed convolution is not a true inverse operation to convolution. Instead, it uses a process that expands the spatial dimensions.</p>
<p>Let’s explore a basic example of transposed convolution with <code class="docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a ConvTranspose2D layer to increase spatial dimensions</span>
<span class="n">torch_transpose_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Disable gradient computation as we’re not training a model</span>
<span class="n">torch_transpose_conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fill the kernel with the edge detection kernel</span>
<span class="n">torch_transpose_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply the transposed convolution to the image tensor</span>
<span class="n">out_transpose_conv</span> <span class="o">=</span> <span class="n">torch_transpose_conv</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out_transpose_conv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 323, 483])
</pre></div>
</div>
</div>
</div>
<p>We observe that the spatial resolution of the output tensor (i.e., <span class="math notranslate nohighlight">\(323 \times 483\)</span>) is slightly larger than the input tensor (<span class="math notranslate nohighlight">\(321 \times 481\)</span>). In general, for an input size of <span class="math notranslate nohighlight">\(n_h \times n_w\)</span> and a kernel size of <span class="math notranslate nohighlight">\(k_h \times k_w\)</span>, the output size will be <span class="math notranslate nohighlight">\((n_h + k_h -1) \times (n_w + k_w -1)\)</span>.</p>
<p>Parameters like <code class="docutils literal notranslate"><span class="pre">padding</span></code> and <code class="docutils literal notranslate"><span class="pre">stride</span></code> also impact transposed convolution but in the opposite way compared to standard convolution. Here, they are applied to the output rather than the input, so setting <code class="docutils literal notranslate"><span class="pre">stride=2</span></code> increases the spatial resolution by a factor of 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a ConvTranspose2D layer with stride=2 to increase spatial dimensions</span>
<span class="n">torch_transpose_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># Disable gradient computation</span>
<span class="n">torch_transpose_conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Fill the kernel with the edge detection kernel</span>
<span class="n">torch_transpose_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">edge_kernel</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply the transposed convolution to the image tensor</span>
<span class="n">out_transpose_conv</span> <span class="o">=</span> <span class="n">torch_transpose_conv</span><span class="p">(</span><span class="n">torch_tensors</span><span class="p">)</span>

<span class="c1"># Print the shape of the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out_transpose_conv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1, 643, 963])
</pre></div>
</div>
</div>
</div>
</section>
<section id="discussion">
<h2>Discussion<a class="headerlink" href="#discussion" title="Link to this heading">#</a></h2>
<p>The discovery of <strong>simple cells</strong> in the visual cortex by Nobel Prize laureates <a class="reference external" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC1363130/pdf/jphysiol01298-0128.pdf">Hubel and Wiesel (1962)</a> marked a major breakthrough in our understanding of how the brain interprets visual information. They demonstrated that specific neurons in the <strong>primary visual cortex (V1)</strong> respond selectively to certain patterns within the visual field, such as edges, lines, or specific orientations. <strong>Convolutional kernels</strong> such as <strong>Gabor</strong> and <strong>Difference-of-Gaussian (DoG)</strong> filters effectively model these <strong>receptive fields</strong> and early stages of visual processing.</p>
<p>These biologically inspired models have not only enhanced our understanding of the visual system but have also influenced the design of <strong>CNNs</strong> that <a class="reference external" href="https://link.springer.com/content/pdf/10.1007/BF00344251.pdf">mimic the process of visual recognition in the brain</a>, progressively extracting hierarchical features from images. This transformation of visual information aligns with the layered processing seen in biological vision.</p>
<p>To see these ideas in action, an <strong>interesting exercise</strong> is to visualise the initial convolutional kernels in different neural network architectures, such as <strong>AlexNet</strong>, <strong>ResNet</strong>, and <strong>Vision Transformers (ViT)</strong>. By comparing these kernels to the receptive field models in biological vision, such as oriented Gabors or double-opponent DoG filters, we can observe how neural networks align well with the properties of the visual cortex.</p>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<p>Practise what we’ve covered with the following exercises:</p>
<ol class="arabic simple">
<li><p>Implement image blurring using <code class="docutils literal notranslate"><span class="pre">torch</span></code>.</p></li>
<li><p>Convert a colour image to greyscale by applying a convolution with <code class="docutils literal notranslate"><span class="pre">kernel_size=(1,</span> <span class="pre">1)</span></code></p></li>
</ol>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">Animated Visualisation of Convolutional Operation</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_computer-vision/transposed-conv.html">Transposed Convolution Explanation</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="networks_building_blocks.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">1. Network’s Building Blocks</p>
      </div>
    </a>
    <a class="right-next"
       href="activation_function.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">1.2. Activation Functions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation">0. Preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#required-packages">Required Packages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-image">Input Image</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualising-rgb-channels">Visualising RGB Channels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#opencv">1. OpenCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-detection">Edge Detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-blurring">Image Blurring</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">2. PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-tensors">Working with Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-detection-with-pytorch">Edge Detection with PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualise-output-tensors">Visualise Output Tensors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-size-in-convolutions">3. Kernel Size in Convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#times-1-convolution"><span class="math notranslate nohighlight">\(1 \times 1\)</span> Convolution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-in-convolution">4. Padding in Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#downsampling">5. Downsampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride">Stride</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dilation">6. Dilation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transposed-convolution">7. Transposed convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Arash Akbarinia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>