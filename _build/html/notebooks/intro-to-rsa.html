
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7.4. Representational Similarity Analysis &#8212; Deep Learning for Experimental Psychologists and Cognitive Neuroscientists</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BXYWD71FWS"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BXYWD71FWS');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BXYWD71FWS');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/intro-to-rsa';</script>
    <link rel="icon" href="../_static/icon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Big Projects" href="big_projects.html" />
    <link rel="prev" title="7.3. Probing by linear classifiers" href="linear_classifier_probe.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Deep Learning for Experimental Psychologists and Cognitive Neuroscientists - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Deep Learning for Experimental Psychologists and Cognitive Neuroscientists - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Prerequisites</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../markdowns/environment_setup.html">0. Environment Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="networks_building_blocks.html">1. Network’s Building Blocks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="convolution.html">1.1. Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="activation_function.html">1.2. Activation Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="pooling.html">1.3. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear.html">1.4. Linear Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="normalisation.html">1.5. Normalisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dropout.html">1.6. Dropout</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">2. Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimisation_learning.html">3. Optimisation and Learning</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="vision.html">4. Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="image_classification.html">4.1. Image Classification</a></li>





<li class="toctree-l2"><a class="reference internal" href="image_segmentation.html">4.2. Image Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="transfer_learning.html">4.3. Transfer Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="other_modalities.html">5. Other Modalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="audio_classification.html">5.1. Audio Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="text_classification.html">5.2. Text Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="clip.html">5.3. Language – Vision</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="generative_models.html">6. Deep Generative Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="gan.html">6.1. Generative Adversarial Network</a></li>






<li class="toctree-l2"><a class="reference internal" href="vae.html">6.2. Deep Autoencoder</a></li>







<li class="toctree-l2"><a class="reference internal" href="dpm.html">6.3. Diffusion Probabilistic Model</a></li>






<li class="toctree-l2"><a class="reference internal" href="llm.html">6.4. Large Language Model</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="interpretation_techniques.html">7. Interpretation Techniques</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="activation.html">7.1. Activation Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesion.html">7.2. Kernel Lesioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_classifier_probe.html">7.3. Probing by linear classifiers</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">7.4. Representational Similarity Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="big_projects.html">8. Big Projects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../markdowns/python_scripting.html">8.1. Python Scripting</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorboard.html">8.2. TensorBoard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../markdowns/server.html">8.3. Working with Servers</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">9. Reinforcement Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="assignments/warmup.html">Warming-up</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/optimisation_learning.html">Optimisation and Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/llm_assignment.html">LLM Calculator</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/zeroshot_evaluation.html">Zero-shot Evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="student_projects/deep-learning-with-dobble.html">Deep Learning with Dobble</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Complementary Materials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="python_course/beginners.html">Python For Beginners</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="python_course/dataTypes.html">Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/modules.html">Modules and NumPy Arrays</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/conditions.html">Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/loops.html">Loops</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/plotting.html">Plotting</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/functions.html">Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/moduleObjects.html">Modules and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_course/inheritance.html">Inheritance</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/blob/master/notebooks/intro-to-rsa.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/edit/master//notebooks/intro-to-rsa.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DeepLearning-JupyterBook/deeplearning-jupyterbook.github.io/issues/new?title=Issue%20on%20page%20%2Fnotebooks/intro-to-rsa.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/intro-to-rsa.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>7.4. Representational Similarity Analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-rsa">Why RSA?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-preparations">Prerequisites &amp; Preparations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-device">Compute Device</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-networks">Downloading Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cifar10">CIFAR10</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-comparing-network-representations">Case Study: Comparing Network Representations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">Outline</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-0-collecting-data">Step 0: Collecting Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-computing-pairwise-dissimilarities">Step 1: Computing Pairwise Dissimilarities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-comparing-representations">Step 2: Comparing Representations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-material">Additional Material</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#literature">Literature</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#foundations-reviews">Foundations &amp; Reviews</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-methods-inference">Statistical Methods &amp; Inference</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-challenges">Applications &amp; Challenges</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-libraries-tutorials">Tools, Libraries &amp; Tutorials</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="representational-similarity-analysis">
<h1>7.4. Representational Similarity Analysis<a class="headerlink" href="#representational-similarity-analysis" title="Link to this heading">#</a></h1>
<p><em>by <a class="reference external" href="https://mrvnthss.github.io">Marvin Theiss</a></em></p>
<p><a class="reference external" href="https://github.com/mrvnthss/intro-to-rsa"><img alt="Static Badge" src="https://img.shields.io/badge/intro--to--rsa-black?style=flat&amp;logo=github&amp;labelColor=000000&amp;color=000000" /></a></p>
<p><em>Version 1.0</em></p>
<p><em>December 2024</em></p>
<p><strong>License</strong>: <a class="reference external" href="https://www.gnu.org/licenses/gpl-3.0.html">GNU General Public License v3</a></p>
<hr class="docutils" />
<p>This notebook is designed to be a <strong>beginner-friendly introduction to representational similarity analysis</strong> (RSA) that does <em>not</em> assume any prior knowledge of or familiarity with RSA. In this notebook, I walk you through the entire process of performing a basic representational similarity analysis. In particular, you will learn …</p>
<ul class="simple">
<li><p>how to <strong>collect intermediate activations</strong> from neural networks (e.g., ResNet-50, ViT-B/16) using PyTorch,</p></li>
<li><p>how to <strong>compute representational dissimilarity matrices</strong> (RDMs) from these activations, and</p></li>
<li><p>how to <strong>compare multiple RDMs</strong> to each other.</p></li>
</ul>
<p>Along the way, you will also learn to <strong>visualize the results</strong> of your analysis. Additional exercises are provided at the end of this notebook to help you become more comfortable with RSA.</p>
<p>By the end of this notebook, you should be able to <strong>interpret the main results of studies that use RSA</strong> as part of their analyses, and you should <strong>feel comfortable performing a basic analysis yourself</strong>. If you want to learn more about RSA after finishing this notebook, I have compiled a list of research articles at the very end of this notebook that you can explore to get started.</p>
<section id="why-rsa">
<h2>Why RSA?<a class="headerlink" href="#why-rsa" title="Link to this heading">#</a></h2>
<p>Understanding how the brain encodes and processes information is one of the central challenges in cognitive neuroscience. A growing body of research suggests that the brain does not represent information in discrete, isolated regions, but rather through distributed patterns of neural activity (Haxby et al., <a class="reference external" href="https://doi.org/10.1126/science.1063736">2000</a>; Mesulam, <a class="reference external" href="https://doi.org/10.1002/ana.410280502">1990</a>; Sporns et al., <a class="reference external" href="https://doi.org/10.1371/journal.pcbi.0010042">2005</a>). This view has led to the development of multivariate analysis techniques that can assess the complex relationships between patterns of neural activation. One such technique is <strong>Representational Similarity Analysis</strong> (RSA), which studies the structure of neural representations by comparing the similarity of activity patterns across different experimental conditions or stimuli (Kriegeskorte et al., <a class="reference external" href="https://doi.org/10.3389/neuro.06.004.2008">2008</a>).</p>
<p>RSA was introduced by Kriegeskorte et al. (<a class="reference external" href="https://doi.org/10.3389/neuro.06.004.2008">2008</a>) as a way to <strong>compare representational geometries</strong> (how neural activation patterns for different stimuli relate to each other) in high-dimensional brain data, such as functional MRI (fMRI). Rather than focusing on the activity of individual voxels, RSA examines the pattern of activity across brain areas (i.e., multiple voxels) to reveal how the brain organizes information. Specifically, RSA is a two-step process that involves (1) calculating the dissimilarity between activation patterns across conditions or stimuli, and (2) comparing these dissimilarities to those predicted by computational models, stimulus properties, or behavior (Kriegeskorte et al., <a class="reference external" href="https://doi.org/10.3389/neuro.06.004.2008">2008</a>; Nili et al., <a class="reference external" href="https://doi.org/10.1371/journal.pcbi.1003553">2014</a>).</p>
<div style="text-align: center">
    <figure>
        <img src="https://github.com/mrvnthss/intro-to-rsa/blob/main/imgs/comparing-representations.png?raw=true" alt="comparing-representations" width="700">
        <br>
        <figcaption style="text-align: center">
            RSA allows to compare pairwise dissimilarities between activity patterns across different systems.
        </figcaption>
    </figure>
</div>
<p>The major methodological advantage of RSA is its <strong>ability to abstract away the spatial dimension</strong> of the data to which it is applied (e.g., fMRI data, computational model outputs). In doing so, it avoids what is known in systems neuroscience as the <strong>spatial correspondence problem</strong>: Which unit in the measured brain data (e.g., individual voxels) corresponds to which unit in the computational model that is being studied? Note that RSA does not <em>solve</em> this problem, it just <em>avoids</em> it. The authors themselves refer to this as the “representational dissimilarity trick” in a later review of their technique (Kriegeskorte &amp; Kievit, <a class="reference external" href="https://doi.org/10.1016/j.tics.2013.06.007">2013</a>). Technically speaking, RSA studies <strong>second-order isomorphisms</strong> (Shepard and Chipman, <a class="reference external" href="https://doi.org/10.1016/0010-0285%2870%2990002-2">1970</a>) instead of first-order isomorphisms. While <em>first-order isomorphisms</em> capture the relationship between stimuli and their representations (How is a given stimulus represented?), <em>second-order isomorphisms</em> attempt to establish a relationship between these first-order isomorphisms (Are two stimuli represented similarly?).</p>
<p>The flexibility and broad applicability of RSA has led to its <strong>widespread adoption in cognitive neuroscience</strong>. RSA is increasingly being used to study various aspects of neural coding, including how the brain represents abstract concepts, processes dynamic sensory input, and organizes categorical information. If you’re curious about how RSA works but have no prior knowledge of it, this notebook is for you!</p>
</section>
<section id="prerequisites-preparations">
<h2>Prerequisites &amp; Preparations<a class="headerlink" href="#prerequisites-preparations" title="Link to this heading">#</a></h2>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Link to this heading">#</a></h3>
<p>The following packages (listed in alphabetical order) need to be installed to run this notebook:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://matplotlib.org/stable/">Matplotlib</a> is a comprehensive library for creating static, animated, and interactive visualizations.</p></li>
<li><p><a class="reference external" href="https://numpy.org">NumPy</a> is a package for scientific computing with Python.</p></li>
<li><p><a class="reference external" href="https://pytorch.org">PyTorch</a> is an open source machine learning framework.</p></li>
<li><p><a class="reference external" href="https://scipy.org">SciPy</a> provides fundamental algorithms for scientific computing in Python.</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/index.html">scikit-learn</a> is a Python module for machine learning built on top of SciPy.</p></li>
<li><p><a class="reference external" href="https://seaborn.pydata.org/index.html">Seaborn</a> is a Python data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/vision/stable/index.html">Torchvision</a> consists of popular datasets, model architectures, and common image transformations for computer vision.</p></li>
</ul>
<p>Let’s import these packages first. When doing so, we follow the <a class="reference external" href="https://peps.python.org/pep-0008/#imports">standard practice</a> of grouping imports in the following order:</p>
<ol class="arabic simple">
<li><p>standard library imports</p></li>
<li><p>related third party imports</p></li>
<li><p>local application/library specific imports (not applicable here)</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpl</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">Normalize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">percentileofscore</span><span class="p">,</span> <span class="n">spearmanr</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">MDS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.feature_extraction</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_feature_extractor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">v2</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="compute-device">
<h3>Compute Device<a class="headerlink" href="#compute-device" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
    <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> 
    <span class="s1">&#39;mps&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> 
    <span class="s1">&#39;cpu&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="downloading-networks">
<h3>Downloading Networks<a class="headerlink" href="#downloading-networks" title="Link to this heading">#</a></h3>
<p>Before we get started, we download all the pre-trained networks that we’ll use in this tutorial. These are:</p>
<ul class="simple">
<li><p><strong>AlexNet</strong> from <a class="reference external" href="https://arxiv.org/abs/1404.5997">One weird trick for parallelizing convolutional neural networks</a>, originally proposed in <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></p></li>
<li><p><strong>ResNet-50</strong> from <a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a></p></li>
<li><p><strong>VGG11</strong> from <a class="reference external" href="https://arxiv.org/pdf/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p></li>
<li><p><strong>ViT-B/16</strong> from <a class="reference external" href="https://openreview.net/pdf?id=YicbFdNTTy">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># AlexNet</span>
<span class="n">alexnet_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">AlexNet_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span>
<span class="n">alexnet</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">alexnet_weights</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">alexnet_trafo</span> <span class="o">=</span> <span class="n">alexnet_weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># ResNet-50</span>
<span class="n">resnet50_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V2</span>
<span class="n">resnet50</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">resnet50_weights</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">resnet50_trafo</span> <span class="o">=</span> <span class="n">resnet50_weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># VGG11</span>
<span class="n">vgg11_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">VGG11_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span>
<span class="n">vgg11</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vgg11</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">vgg11_weights</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vgg11_trafo</span> <span class="o">=</span> <span class="n">vgg11_weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># ViT-B/16</span>
<span class="n">vit_b16_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ViT_B_16_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span>
<span class="n">vit_b16</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">vit_b16_weights</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vit_b16_trafo</span> <span class="o">=</span> <span class="n">vit_b16_weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The following block of code is <strong>only relevant for Mac users</strong> and is necessary due to the fact that, at the time of writing this tutorial, the <code class="docutils literal notranslate"><span class="pre">aten::_upsample_bilinear2d_aa.out</span></code> operator (which is part of <em>all</em> transforms in the code block above) was not implemented for <strong>MPS</strong>. To make matters worse, the recommended temporary fix of setting the environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_ENABLE_MPS_FALLBACK=1</span></code> to use the CPU as a fallback did not work either. Hence, when using MPS, we move all tensors to the CPU first, then apply the transforms, and then move the tensors back to the GPU. That way, we can at least take advantage of MPS when performing the forward passes through the networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;mps&#39;</span><span class="p">:</span>
    <span class="c1"># Define transforms to move tensors across devices</span>
    <span class="n">to_cpu</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
    <span class="n">to_mps</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">))</span>

    <span class="c1"># Modify preset transforms</span>
    <span class="n">alexnet_trafo</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">to_cpu</span><span class="p">,</span> <span class="n">alexnet_trafo</span><span class="p">,</span> <span class="n">to_mps</span><span class="p">])</span>
    <span class="n">resnet50_trafo</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">to_cpu</span><span class="p">,</span> <span class="n">resnet50_trafo</span><span class="p">,</span> <span class="n">to_mps</span><span class="p">])</span>
    <span class="n">vgg11_trafo</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">to_cpu</span><span class="p">,</span> <span class="n">vgg11_trafo</span><span class="p">,</span> <span class="n">to_mps</span><span class="p">])</span>
    <span class="n">vit_b16_trafo</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">to_cpu</span><span class="p">,</span> <span class="n">vit_b16_trafo</span><span class="p">,</span> <span class="n">to_mps</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="cifar10">
<h3>CIFAR10<a class="headerlink" href="#cifar10" title="Link to this heading">#</a></h3>
<p>For this introductory tutorial, we will use images from the <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> dataset. To get started, we download the dataset from <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Equivalent to ToTensor() in v1 of torchvision.transforms</span>
<span class="n">to_tensor</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">v2</span><span class="o">.</span><span class="n">ToImage</span><span class="p">(),</span>
    <span class="n">v2</span><span class="o">.</span><span class="n">ToDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Download CIFAR10 test set (1,000 images per class)</span>
<span class="n">cifar10</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">to_tensor</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Files already downloaded and verified
</pre></div>
</div>
</div>
</div>
<p>We won’t be using the CIFAR10 dataset in its entirety. Instead, we only select a few images per class to use in the RSA analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Choose number of images to use from each class</span>
<span class="n">num_images_per_class</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Create dictionary to store selected images</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cifar10</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span>
<span class="n">images_by_class</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)}</span>

<span class="c1"># Shuffle CIFAR10 dataset</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cifar10</span><span class="p">)))</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

<span class="c1"># Select first 10 images (after shuffling) of each class</span>
<span class="n">subset_indices</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">cifar10</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">images_by_class</span><span class="p">[</span><span class="n">label</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">images_by_class</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">img</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
        <span class="n">subset_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

<span class="c1"># Flatten images and labels into single tensors each</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">images_by_class</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">])</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us take a look at the images that we’ll be using from here on out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">num_images_per_class</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">num_images_per_class</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_images_per_class</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">num_images_per_class</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5f0aaba88877ed46db68bbc2816d6b1cb065a5767fd9d1e411d75412ae4f2e2c.png" src="../_images/5f0aaba88877ed46db68bbc2816d6b1cb065a5767fd9d1e411d75412ae4f2e2c.png" />
</div>
</div>
<p>Finally, let’s store the selected images along with their labels in a dictionary for later use.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cifar10_mini</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;images&#39;</span><span class="p">:</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="case-study-comparing-network-representations">
<h2>Case Study: Comparing Network Representations<a class="headerlink" href="#case-study-comparing-network-representations" title="Link to this heading">#</a></h2>
<p>To illustrate how RSA can be used to compare representational geometries, we focus on the simple setup of <strong>comparing multiple deep neural networks</strong> to each other. That way, we avoid the additional work that is required when dealing with fMRI data (preprocessing, averaging across participants, etc.). In particular, we will compare the representations learned by the <strong>penultimate layers</strong> of four different network architectures: AlexNet, ResNet-50, VGG11, and ViT-B/16. Note that the first three are all <strong>convolutional neural networks</strong> (CNNs), whereas ViT-B/16 is based on the <strong>transformer architecture</strong>. Do all three CNNs share similar representations? If so, is it different to the one learned by ViT-B/16? Let’s find out!</p>
<section id="outline">
<h3>Outline<a class="headerlink" href="#outline" title="Link to this heading">#</a></h3>
<p>As mentioned in the introduction of this notebook, RSA is a <strong>two-step process</strong>. In the <strong>first step</strong>, we compare the activation patterns elicited by two systems in response to the same set of stimuli within each system. Precisely, for each system (e.g., brain, computational model) we compare the activity pattern elicited by the <span class="math notranslate nohighlight">\(i\)</span>-th stimulus to the activity pattern of the <span class="math notranslate nohighlight">\(j\)</span>-th stimulus for all distinct pairs of stimuli <span class="math notranslate nohighlight">\(i \neq j\)</span>. These pairwise dissimilarities are then arranged in a square matrix, called the <em>representational dissimilarity matrix</em>, which is indexed by the stimuli that were used to probe the system. In the <strong>second step</strong>, these matrices (one for each system) are then quantitatively compared to each other. This process is visualized in the figure below.</p>
<div style="text-align: center">
    <figure>
        <img src="https://github.com/mrvnthss/intro-to-rsa/blob/main/imgs/rsa-pipeline.png?raw=true" alt="rsa-pipeline" width="700">
        <br>
        <figcaption style="text-align: center">
            A schematic visualization of the representational similarity analysis pipeline.
        </figcaption>
    </figure>
</div>
</section>
<section id="step-0-collecting-data">
<h3>Step 0: Collecting Data<a class="headerlink" href="#step-0-collecting-data" title="Link to this heading">#</a></h3>
<p>Every RSA analysis will require <strong>data collection</strong>. Depending on the systems that are to be compared, this can be more or less expensive and time-consuming. If one is interested in analyzing <strong>DNNs</strong>, data collection amounts to a simple forward pass through the network, which is computationally cheap (assuming that pre-trained networks are used) and requires little time. Similarly, if one wants to analyze <strong>brain data</strong> (e.g., fMRI, EEG), there are several datasets available online that are ready-to-use (e.g., <a class="reference external" href="https://www.nature.com/articles/s41593-021-00962-x">Natural Scenes Dataset</a>, <a class="reference external" href="https://things-initiative.org">THINGS datasets</a>). On the contrary, if one wants to analyze a new DNN architecture that needs to be trained from scratch or one wants to record new brain data, data collection can quickly become time-consuming and expensive.</p>
<div style="text-align: center">
    <figure>
        <img src="https://github.com/mrvnthss/intro-to-rsa/blob/main/imgs/collecting-activations.png?raw=true" alt="collecting-activations" width="300">
        <br>
        <figcaption style="text-align: center">
            Collecting activations to serve as input to the RSA analysis.
        </figcaption>
    </figure>
</div>
<p>In this tutorial, we will restrict ourselves to comparing the representations learned by different DNNs that have all been trained on the <a class="reference external" href="https://ieeexplore.ieee.org/document/5206848">ImageNet</a> dataset. To <strong>extract intermediate activations</strong>, we use the <code class="docutils literal notranslate"><span class="pre">create_feature_extractor</span></code> function. We start by extracting intermediate activations of <strong>AlexNet</strong>.</p>
<p><strong>Note</strong>: AlexNet outputs a tensor of length 1,000 as it was trained on ImageNet (which has 1,000 classes). If we wanted to perform classification on the CIFAR10 dataset, we would have to replace the final linear layer with one that outputs a vector of length 10, and then fine-tune this layer on CIFAR10. However, we are solely interested in the internal representations (features) that AlexNet has learned from being trained on ImageNet, and we probe this learned representation by using images from the CIFAR10 dataset. Hence, we can use AlexNet <em>as is</em>, without any modifications to its architecture or weights. The same applies to the remaining architectures.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create feature extractor</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">create_feature_extractor</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">alexnet</span><span class="p">,</span>
    <span class="n">return_nodes</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;classifier.5&#39;</span><span class="p">:</span> <span class="s1">&#39;penultimate_layer&#39;</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Perform forward pass with appropriate transform</span>
<span class="c1"># NOTE: We detach the output from the computational graph, as we won&#39;t need any gradients later on.</span>
<span class="n">alexnet_activations</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span>
    <span class="n">alexnet_trafo</span><span class="p">(</span><span class="n">cifar10_mini</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">])</span>
<span class="p">)[</span><span class="s1">&#39;penultimate_layer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s inspect the activations that we have extracted to make sure that everything works as expected. The penultimate layer of AlexNet outputs a vector of length 4,096. Since we passed a batch of 100 images to the network, we expect to extract a tensor of shape <span class="math notranslate nohighlight">\(100 \times 4096\)</span>. Indeed, this is what we end up with:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">alexnet_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([100, 4096])
</pre></div>
</div>
</div>
</div>
<p>Let’s repeat this step for the <strong>remaining networks</strong>. We store all activations in a single dictionary, as this makes it easier to work with the data later on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up dictionary to store all activations</span>
<span class="n">activations</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;alexnet&#39;</span><span class="p">:</span> <span class="n">alexnet_activations</span>
<span class="p">}</span>

<span class="c1"># Set up variables to loop over</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">resnet50</span><span class="p">,</span> <span class="n">vgg11</span><span class="p">,</span> <span class="n">vit_b16</span><span class="p">]</span>
<span class="n">return_nodes</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;avgpool&#39;</span><span class="p">:</span> <span class="s1">&#39;penultimate_layer&#39;</span><span class="p">},</span>       <span class="c1"># ResNet</span>
    <span class="p">{</span><span class="s1">&#39;classifier.4&#39;</span><span class="p">:</span> <span class="s1">&#39;penultimate_layer&#39;</span><span class="p">},</span>  <span class="c1"># VGG</span>
    <span class="p">{</span><span class="s1">&#39;encoder.ln&#39;</span><span class="p">:</span> <span class="s1">&#39;penultimate_layer&#39;</span><span class="p">}</span>     <span class="c1"># ViT</span>
<span class="p">]</span>
<span class="n">trafos</span> <span class="o">=</span> <span class="p">[</span><span class="n">resnet50_trafo</span><span class="p">,</span> <span class="n">vgg11_trafo</span><span class="p">,</span> <span class="n">vit_b16_trafo</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;resnet50&#39;</span><span class="p">,</span> <span class="s1">&#39;vgg11&#39;</span><span class="p">,</span> <span class="s1">&#39;vit_b16&#39;</span><span class="p">]</span>

<span class="c1"># Loop over all models</span>
<span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">return_nodes</span><span class="p">,</span> <span class="n">trafos</span><span class="p">,</span> <span class="n">names</span><span class="p">):</span>
    <span class="c1"># Create feature extractor</span>
    <span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">create_feature_extractor</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
        <span class="n">return_nodes</span><span class="o">=</span><span class="n">r</span>
    <span class="p">)</span>

    <span class="c1"># Perform forward pass and store activations</span>
    <span class="n">activations</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span>
        <span class="n">t</span><span class="p">(</span><span class="n">cifar10_mini</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">])</span>
    <span class="p">)[</span><span class="s1">&#39;penultimate_layer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># no need for gradients here</span>
</pre></div>
</div>
</div>
</div>
<p>Again, let’s make sure that all activation tensors are as expected.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>alexnet: torch.Size([100, 4096])
resnet50: torch.Size([100, 2048, 1, 1])
vgg11: torch.Size([100, 4096])
vit_b16: torch.Size([100, 197, 768])
</pre></div>
</div>
</div>
</div>
<p>Note two things here.</p>
<p><strong>First</strong>, the activation tensor produced by <strong>ResNet-50</strong> carries two additional singleton dimensions (i.e., dimensions of size 1). This results from the fact that we extract the activations produced by an <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html"><strong>AdaptiveAvgPool2d</strong></a> layer with output size <span class="math notranslate nohighlight">\((1, 1)\)</span>. Effectively, this layer takes as input a tensor of shape <span class="math notranslate nohighlight">\((B, C, H, W)\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(C\)</span> is the number of channels, and <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(W\)</span> are the spatial dimensions, and then averages across the entire spatial resolution for each image and each channel. In our example, this reduces a tensor of shape <span class="math notranslate nohighlight">\((100, 2048, 7, 7)\)</span> to a tensor of shape <span class="math notranslate nohighlight">\((100, 2048, 1, 1)\)</span>. To remove the redundant dimensions, we simply apply the <code class="docutils literal notranslate"><span class="pre">squeeze</span></code> function available in PyTorch.</p>
<p><strong>Second</strong>, the activation tensor produced by <strong>ViT-B/16</strong> has a different structure compared to the three CNNs altogether. This is because the Vision Transformer (ViT) architecture processes images as sequences of individual patches rather than as a single grid of pixels. More precisely, ViT divides an input image into a sequence of fixed-size patches, which are then linearly embedded into vectors. These vectors are concatenated with a <em>special classification token</em> (CLS token) and positional embeddings (these essentially let ViT know which image patch belongs where in the original image), forming the input sequence to the transformer encoder. The output of the transformer encoder is a sequence of vectors, where the first vector corresponds to the CLS token. Hence, we end up with a tensor of shape <span class="math notranslate nohighlight">\((B, N, D)\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(N\)</span> is one more than the number of patches (due to the additional CLS token), and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension. In our case, we have <span class="math notranslate nohighlight">\(B = 100\)</span> images and <span class="math notranslate nohighlight">\(N = 197\)</span> vectors of length <span class="math notranslate nohighlight">\(D = 768\)</span>. The ViT-B/16 architecture uses patches of size <span class="math notranslate nohighlight">\(16 \times 16\)</span>. Since we need <span class="math notranslate nohighlight">\((224 / 16)^2 = 196\)</span> of these to cover the entire image, we end up with <span class="math notranslate nohighlight">\(N = 196 + 1\)</span> vectors. The CLS token is used for classification tasks and represents the entire image. For that reason, we extract the activations corresponding to the CLS token for further analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activations</span><span class="p">[</span><span class="s1">&#39;resnet50&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="s1">&#39;resnet50&#39;</span><span class="p">])</span>  <span class="c1"># remove singleton dimensions</span>
<span class="n">activations</span><span class="p">[</span><span class="s1">&#39;vit_b16&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">&#39;vit_b16&#39;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># extract CLS token</span>

<span class="c1"># Let&#39;s check again</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>alexnet: torch.Size([100, 4096])
resnet50: torch.Size([100, 2048])
vgg11: torch.Size([100, 4096])
vit_b16: torch.Size([100, 768])
</pre></div>
</div>
</div>
</div>
<p>That’s it! We have all the data we need to perform our representational similarity analysis! Next, we need to compute the pairwise dissimilarities of the activations produced by each network, to capture each network’s <em>representational geometry</em>.</p>
</section>
<section id="step-1-computing-pairwise-dissimilarities">
<h3>Step 1: Computing Pairwise Dissimilarities<a class="headerlink" href="#step-1-computing-pairwise-dissimilarities" title="Link to this heading">#</a></h3>
<p>To capture the representational geometries of the networks we have probed (more precisely, of their penultimate layers), we need to compute the <strong>representational dissimilarity matrix (RDM)</strong> produced by each network. The RDM is a square <span class="math notranslate nohighlight">\(N \times N\)</span> matrix, where <span class="math notranslate nohighlight">\(N\)</span> is the number of stimuli that was used to probe the network (or, more general, the system we’re interested in). The entry at position <span class="math notranslate nohighlight">\((i, j)\)</span> for <span class="math notranslate nohighlight">\(i, j \in \{1, \dots, N\}\)</span> is the dissimilarity <span class="math notranslate nohighlight">\(d(\mathbf{x}_i, \mathbf{x}_j)\)</span> between the activity patterns produced by the <span class="math notranslate nohighlight">\(i\)</span>-th and <span class="math notranslate nohighlight">\(j\)</span>-th stimulus. This dissimilarity is quantified using a <em>distance function</em> <span class="math notranslate nohighlight">\(d\)</span>. This function does not necessarily have to be a distance function in the mathematical sense (see <a class="reference external" href="https://en.wikipedia.org/wiki/Distance#Mathematical_formalization">here</a>), but it is usually assumed to be <em>symmetric</em>, meaning that it outputs the same value irrespective of the order of the arguments being passed to it, i.e., <span class="math notranslate nohighlight">\(d(\mathbf{x}_i, \mathbf{x}_j) = d(\mathbf{x}_j, \mathbf{x}_i)\)</span>. Additionally assuming that the distance from a point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to itself is always zero (i.e., <span class="math notranslate nohighlight">\(d(\mathbf{x}, \mathbf{x}) = 0\)</span>), an RDM is a <a class="reference external" href="https://en.wikipedia.org/wiki/Symmetric_matrix">symmetric matrix</a> with <span class="math notranslate nohighlight">\(0\)</span>s on the diagonal as depicted below.</p>
<div style="text-align: center">
    <figure>
        <img src="https://github.com/mrvnthss/intro-to-rsa/blob/main/imgs/pairwise-dissimilarities.png?raw=true" alt="pairwise-dissimilarities" width="700">
        <br>
        <figcaption style="text-align: center">
            A <i>representational dissimilarity matrix</i> (RDM) quantifies the dissimilarities between activations for pairs of stimuli.
        </figcaption>
    </figure>
</div>
<p>For this tutorial, we will use <strong>Euclidean distance</strong> to compute the RDMs. Be aware that there exist many more distances that are commonly used in practice, and that the choice of the appropriate metric is an important task. Check the further readings section at the end of this notebook, if you’re interested!</p>
<p>Recall that the Euclidean distance <span class="math notranslate nohighlight">\(d_2\)</span> between two column vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is defined as the square root of the norm of the difference <span class="math notranslate nohighlight">\(\mathbf{x} - \mathbf{y}\)</span> of the two vectors, i.e.,</p>
<div class="math notranslate nohighlight">
\[
d_2(\mathbf{x}, \mathbf{y}) = \lVert \mathbf{x} - \mathbf{y} \rVert_2 = \sqrt{\langle \mathbf{x} - \mathbf{y}, \mathbf{x} - \mathbf{y}\rangle} = \sqrt{\sum_{k=1}^M (x_k - y_k)^2}, \quad \mathbf{x}, \mathbf{y} \in \mathbb{R}^M \,.
\]</div>
<p>In the equation above, <span class="math notranslate nohighlight">\(M\)</span> denotes the dimensionality of the activation vector. Using Euclidean distance, let’s compute the dissimilarity between the activations produced by the first and second image!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We extract the activations ...</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">&#39;alexnet&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="s1">&#39;alexnet&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># ... and compute their Euclidean distance!</span>
<span class="n">d_xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">d_xy</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>79.44
</pre></div>
</div>
</div>
</div>
<p>We now know that the two activity patterns elicited by the first two images are approximately 79 units apart in AlexNet’s representational space. Next, we need to compute all the remaining pairwise dissimilarities. In total, there are <span class="math notranslate nohighlight">\(N (N-1) / 2\)</span> dissimilarities that we have to compute.</p>
<p>We could do so by using a nested for-loop to compute all of these dissimilarities, which <em>could</em> look something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>  <span class="c1"># number of stimuli</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="o">...</span> <span class="n">N</span><span class="p">:</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>

<span class="n">X</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is the representational similarity matrix we want to compute, and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> represents the tensor of activations (e.g., <code class="docutils literal notranslate"><span class="pre">activations['alexnet']</span></code>). While this implementation does take advantage of the fact that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is symmetric, it is still <em>inefficient</em> since the for-loop has to be <em>executed sequentially</em> and cannot be run in parallel.</p>
<p>To speed things up drastically, we can <strong>vectorize our code</strong> to take advantage of the performance boost provided by parallelization. Doing so involves a bit of linear algebra. <strong>Feel free to skip the details!</strong> All we’re doing really is to find a closed-form expression that lets us compute all pairwise distances at once, eliminating the need for a for-loop.</p>
<p>For the math-savvy readers out there, keep on reading. First, observe that the <em>squared</em> Euclidean distance between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> can be rewritten as follows:</p>
<div class="math notranslate nohighlight">
\[
\lVert \mathbf{x} - \mathbf{y} \rVert_2^2 = \langle \mathbf{x} - \mathbf{y}, \mathbf{x} - \mathbf{y} \rangle = \lVert \mathbf{x} \rVert_2^2 + \lVert \mathbf{y} \rVert_2^2 - 2 \mathbf{x}^{\top} \mathbf{y} \,. \tag{1}
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x}^{\top} = \mathbf{A}_{i,:}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}^{\top} = \mathbf{A}_{j,:}\)</span> correspond to two rows of an <span class="math notranslate nohighlight">\(N \times M\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> of activations. Note that <span class="math notranslate nohighlight">\(\mathbf{A}_{i,:}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}_{j,:}\)</span> are row vectors by definition. By setting these equal to <span class="math notranslate nohighlight">\(\mathbf{x}^{\top}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}^{\top}\)</span>, respectively, we stick to our convention of working with column vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. Rewriting (1), we get</p>
<div class="math notranslate nohighlight">
\[
\lVert (\mathbf{A}_{i,:})^{\top} - (\mathbf{A}_{j,:})^{\top} \rVert_2^2 = \lVert (\mathbf{A}_{i,:})^{\top} \rVert_2^2 + \lVert (\mathbf{A}_{j,:})^{\top} \rVert_2^2 - 2 \mathbf{A}_{i,:} (\mathbf{A}_{j,:})^{\top} = \lVert (\mathbf{A}_{i,:})^{\top} \rVert_2^2 + \lVert (\mathbf{A}_{j,:})^{\top} \rVert_2^2 - 2 (\mathbf{A} \mathbf{A}^{\top})_{i,j} \,. \tag{2}
\]</div>
<p>The last identity follows from the definition of matrix multiplication and the observation that the transpose of the <span class="math notranslate nohighlight">\(j\)</span>-th row of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is equal to the <span class="math notranslate nohighlight">\(j\)</span>-th column of <span class="math notranslate nohighlight">\(\mathbf{A}^{\top}\)</span>, i.e., <span class="math notranslate nohighlight">\((\mathbf{A}_{j,:})^{\top} = (\mathbf{A}^{\top})_{:,j}\)</span>. Next comes the slightly creative part. We introduce another matrix <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> whose entry at position <span class="math notranslate nohighlight">\((i, j)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
B_{i,j} = \sum_{k=1}^M (A_{i,k})^2 = \lVert (\mathbf{A}_{i,:})^{\top} \rVert_2^2, \quad i, j \in \{1, \dots, N\} \,. \tag{3}
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(B_{i,j}\)</span> only depends on <span class="math notranslate nohighlight">\(i\)</span>. In other words, <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> is simply a column vector that’s replicated and stacked next to each other <span class="math notranslate nohighlight">\(N\)</span> times. Consequently, <span class="math notranslate nohighlight">\(\mathbf{B}^{\top}\)</span> is a row vector that’s stacked underneath each other multiple times. In particular, we have</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{B}^{\top})_{i,j} = B_{j,i} = \lVert (\mathbf{A}_{j,:})^{\top} \rVert_2^2 \,.
\]</div>
<p>Hence, we can plug the definition of <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> given in (3) into equation (2) to arrive at</p>
<div class="math notranslate nohighlight">
\[
\lVert (\mathbf{A}_{i,:})^{\top} - (\mathbf{A}_{j,:})^{\top} \rVert_2^2 = B_{i,j} + (\mathbf{B}^{\top})_{i,j} - 2 (\mathbf{A} \mathbf{A}^{\top})_{i,j} = (\mathbf{B} + \mathbf{B}^{\top} - 2 \mathbf{A} \mathbf{A}^{\top})_{i,j} \,. \tag{4}
\]</div>
<p>What’s conceptually important here is that the pairwise dissimilarity between any pair of stimuli corresponds to an entry in a single matrix (the RHS of (4)) that we can compute in a single go. That is, the element-wise square root of the matrix on the RHS of (4) is our representational dissimilarity matrix! Since we will be computing RDMs for multiple networks, we implement (4) as a function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compute_rdm</span><span class="p">(</span>
    <span class="n">activations</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">normalize_distances</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute an RDM using (normalized) Euclidean distance.</span>

<span class="sd">    Args:</span>
<span class="sd">        activations: The matrix of activations from which to compute the</span>
<span class="sd">          RDM.  Must be a 2-D tensor of size (N, M), where N is the</span>
<span class="sd">          number of stimuli, and M 2 is the number of unit activations</span>
<span class="sd">          per stimulus.</span>
<span class="sd">        normalize_distances: Whether to normalize the squared pairwise</span>
<span class="sd">          distances (i.e., before taking the square root) by the number</span>
<span class="sd">          M of unit activations per stimulus.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The RDM computed from the data using Euclidean distance as a 2-D</span>
<span class="sd">        tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Compute squared pairwise distances</span>
    <span class="n">norms_squared</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">activations</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># B as defined in (3)</span>
    <span class="n">distances_squared</span> <span class="o">=</span> <span class="n">norms_squared</span> <span class="o">+</span> <span class="n">norms_squared</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">activations</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># RHS of (4)</span>

    <span class="c1"># Clamp values to 0 (to potentially correct for numerical inaccuracies)</span>
    <span class="n">distances_squared</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">distances_squared</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    
    <span class="c1"># Normalize Euclidean distances by number of units M</span>
    <span class="n">rdm</span> <span class="o">=</span> <span class="n">distances_squared</span> <span class="o">/</span> <span class="n">activations</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">normalize_distances</span> <span class="k">else</span> <span class="n">distances_squared</span>

    <span class="c1"># Compute element-wise square root</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rdm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, let’s use this function to <strong>compute the RDMs</strong> for all our networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">raw_rdms</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">activation</span> <span class="ow">in</span> <span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">raw_rdms</span><span class="p">[</span><span class="n">model</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_rdm</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, before we compare these RDMs to each other, let’s visualize them! To do so, we convert the raw entries of the RDMs (i.e., the true dissimilarities) to <strong>percentiles</strong>. This is commonly done in practice since doing so makes it easier to compare multiple RDMs to each other. Additionally, it highlights any structures present in the RDMs. <em>At the same time, one should always be aware of this very fact!</em> That is, it may appear as if there is a very clear and distinct structure present in an RDM even when the true differences between the raw entries of the RDM are very small.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert to percentiles</span>
<span class="n">percentile_rdms</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;alexnet&#39;</span><span class="p">,</span> <span class="s1">&#39;resnet50&#39;</span><span class="p">,</span> <span class="s1">&#39;vgg11&#39;</span><span class="p">,</span> <span class="s1">&#39;vit_b16&#39;</span><span class="p">]:</span>
    <span class="n">raw_rdm</span> <span class="o">=</span> <span class="n">raw_rdms</span><span class="p">[</span><span class="n">model</span><span class="p">]</span>
    <span class="n">percentiles</span> <span class="o">=</span> <span class="n">percentileofscore</span><span class="p">(</span><span class="n">raw_rdm</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">raw_rdm</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;rank&#39;</span><span class="p">)</span>    
    <span class="n">percentile_rdms</span><span class="p">[</span><span class="n">model</span><span class="p">]</span> <span class="o">=</span> <span class="n">percentiles</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">raw_rdm</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Prepare plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Plot RDMs</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s1">&#39;notebook&#39;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;alexnet&#39;</span><span class="p">,</span> <span class="s1">&#39;resnet50&#39;</span><span class="p">,</span> <span class="s1">&#39;vgg11&#39;</span><span class="p">,</span> <span class="s1">&#39;vit_b16&#39;</span><span class="p">]):</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
            <span class="n">percentile_rdms</span><span class="p">[</span><span class="n">model</span><span class="p">],</span>
            <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">vmax</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span>
            <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
        <span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>

    <span class="c1"># Add title</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Representational Dissimilarity Matrices&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add colorbar</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_position</span><span class="p">()</span>
    <span class="n">cbar_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="n">pos</span><span class="o">.</span><span class="n">x1</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.218</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.56</span><span class="p">])</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cax</span><span class="o">=</span><span class="n">cbar_ax</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;percentiles&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=-</span><span class="mi">15</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b179f08854edc3d9638bd1009ce4731e1c2fa260a3fd16a530da7f448e7e2b4b.png" src="../_images/b179f08854edc3d9638bd1009ce4731e1c2fa260a3fd16a530da7f448e7e2b4b.png" />
</div>
</div>
<p>From <strong>visual inspection</strong> alone, it seems as if the representations of AlexNet and VGG11 are quite similar, and so are the representations of ResNet-50 and ViT-B/16. Let’s find out if this also holds analytically in the next section!</p>
</section>
<section id="step-2-comparing-representations">
<h3>Step 2: Comparing Representations<a class="headerlink" href="#step-2-comparing-representations" title="Link to this heading">#</a></h3>
<p>We want to find out whether the <strong>RDMs</strong> produced by two systems (here: two networks) <strong>are similar or not</strong>. Assuming that the distance function that was used to compute the RDMs is symmetric, each RDM is a symmetric matrix with <span class="math notranslate nohighlight">\(0\)</span>s on the diagonal. Hence, all the information characterized by a single RDM is contained in its <strong>upper triangular part</strong>, excluding the diagonal. Hence, we can extract these entries and flatten them into a single vector as illustrated in the figure below.</p>
<div style="text-align: center">
    <figure>
        <img src="https://github.com/mrvnthss/intro-to-rsa/blob/main/imgs/comparing-rdms.png?raw=true" alt="comparing-rdms" width="700">
        <br>
        <figcaption style="text-align: center">
            The representational similarity of two systems is determined by the degree of similarity between the RDMs produced by each system.
        </figcaption>
    </figure>
</div>
<p>Once we have done so, the problem of determining whether two systems produce similar internal representations reduces to quantifying the similarity of these two vectors. Popular measures for this problem are <a class="reference external" href="https://en.wikipedia.org/wiki/Cosine_similarity"><strong>cosine similarity</strong></a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Correlation"><strong>correlation</strong></a> measures. In this notebook, we’ll be using <a class="reference external" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"><strong>rank correlation</strong></a> to determine the similarity of two RDMs.</p>
<p>To get started, we need a <strong>function that lets us extract the upper triangular part</strong> of an RDM and flatten it into a vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_upper_tri_matrix</span><span class="p">(</span><span class="n">sq_matrix</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract the upper triangular part of a square matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        sq_matrix: The square matrix from which to extract the upper</span>
<span class="sd">          triangular matrix (excluding the diagonal).</span>

<span class="sd">    Returns:</span>
<span class="sd">        The upper triangular matrix (excluding the diagonal) of the</span>
<span class="sd">        square matrix ``sq_matrix``, flattened into a vector using</span>
<span class="sd">        row-major order.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sq_matrix</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sq_matrix</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we can compute the <strong>pairwise rank correlation coefficients</strong> between all pairs of RDMs that we have computed in the previous section. There is <strong>one important detail</strong> to be pointed out here: For further analyses and visualization, we keep the quantity <span class="math notranslate nohighlight">\(1 - \text{correlation}\)</span> instead of the (rank) correlation itself. This simple step transforms the (rank) correlation from a <strong>similarity measure</strong> (i.e., higher values indicate higher <em>similarity</em>) into a <strong>dissimilarity measure</strong> (i.e., higher values indicate higher <em>dissimilarity</em>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize matrix to store results</span>
<span class="n">rdm_correlations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Compute all pairwise correlations</span>
<span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="n">rdm1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_rdms</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
    <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">rdm2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_rdms</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">col</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">rdm_correlations</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">spearmanr</span><span class="p">(</span>  <span class="c1"># 1 - correlation</span>
            <span class="n">get_upper_tri_matrix</span><span class="p">(</span><span class="n">rdm1</span><span class="o">.</span><span class="n">cpu</span><span class="p">()),</span>
            <span class="n">get_upper_tri_matrix</span><span class="p">(</span><span class="n">rdm2</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
        <span class="p">)</span><span class="o">.</span><span class="n">statistic</span>

<span class="n">display</span><span class="p">(</span><span class="n">rdm_correlations</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0000, 0.7004, 0.2565, 0.7675],
        [0.7004, 0.0000, 0.6104, 0.3533],
        [0.2565, 0.6104, 0.0000, 0.7012],
        [0.7675, 0.3533, 0.7012, 0.0000]])
</pre></div>
</div>
</div>
</div>
<p>The matrix <code class="docutils literal notranslate"><span class="pre">rdm_correlations</span></code> that we have just computed is essentially a dissimilarity matrix of dissimilarity matrices, which is sometimes referred to as a <strong>second-order dissimilarity matrix</strong>. Just staring at the raw numbers is somewhat unintuitive. Let’s visualize this matrix just like we’ve done for the previous RDMs!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot data</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s1">&#39;notebook&#39;</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
        <span class="n">rdm_correlations</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span>
        <span class="n">square</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    
    <span class="c1"># Add title, modify axes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Second-Order Dissimilarity Matrix&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">raw_rdms</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">raw_rdms</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    
    <span class="c1"># Adjust colorbar</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$1 - \text</span><span class="si">{correlation}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/67a82b2e44a59922c74b68e023da7a3d9d65b8b1b7ff011d867e6a85935fc6cb.png" src="../_images/67a82b2e44a59922c74b68e023da7a3d9d65b8b1b7ff011d867e6a85935fc6cb.png" />
</div>
</div>
<p>These results confirm what we had already assumed after looking at the individual RDMs: AlexNet and VGG11 are quite similar, and so are ResNet-50 and ViT-B/16, while all remaining pairs are less similar to each other. Another way of visualizing these results is to use <a class="reference external" href="https://en.wikipedia.org/wiki/Multidimensional_scaling"><strong>multidimensional scaling</strong></a> (MDS). Multidimensional scaling is a statistical technique used to visualize the similarity or dissimilarity of (potentially high-dimensional) data in a lower-dimensional space by placing each datapoint in the low-dimensional space such that the distances between these points match the true dissimilarities as closely as possible.</p>
<p>For this notebook, we use the implementation of MDS provided by <a class="reference external" href="https://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds">scikit-learn</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply MDS to second-order dissimilarity matrix</span>
<span class="n">mds</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">dissimilarity</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span>
<span class="p">)</span>
<span class="n">embedded_pos</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">rdm_correlations</span><span class="p">)</span>

<span class="c1"># Preparations for plot</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">colormaps</span><span class="p">[</span><span class="s1">&#39;viridis&#39;</span><span class="p">]</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xytexts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>    <span class="c1"># AlexNet</span>
    <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">),</span>  <span class="c1"># ResNet-50</span>
    <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>    <span class="c1"># VGG11</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>     <span class="c1"># ViT-B/16</span>
<span class="p">]</span>

<span class="c1"># Plot results</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s1">&#39;notebook&#39;</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">embedded_pos</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">y</span><span class="o">=</span><span class="n">embedded_pos</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="c1"># Add connecting lines</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">embedded_pos</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embedded_pos</span><span class="p">)):</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">rdm_correlations</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]))</span>
            <span class="n">distance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embedded_pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">embedded_pos</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            <span class="n">line_width</span> <span class="o">=</span> <span class="n">rdm_correlations</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">distance</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                <span class="p">[</span><span class="n">embedded_pos</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embedded_pos</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">embedded_pos</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">embedded_pos</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
                <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                <span class="n">lw</span><span class="o">=</span><span class="n">line_width</span><span class="p">,</span>
                <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>

    <span class="c1"># Add title, modify plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;MDS Embedding of Second-Order Dissimilarity Matrix&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Annotate individual points</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;AlexNet&#39;</span><span class="p">,</span> <span class="s1">&#39;ResNet-50&#39;</span><span class="p">,</span> <span class="s1">&#39;VGG11&#39;</span><span class="p">,</span> <span class="s1">&#39;ViT-B/16&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">embedded_pos</span><span class="p">)):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
            <span class="n">label</span><span class="p">,</span>
            <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
            <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
            <span class="n">xytext</span><span class="o">=</span><span class="n">xytexts</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
            <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span>
        <span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8726e19236530f124f3c78eacbcd8ccb971b9aa57cf56d135890ab313520833d.png" src="../_images/8726e19236530f124f3c78eacbcd8ccb971b9aa57cf56d135890ab313520833d.png" />
</div>
</div>
<p>Once more, this plot confirms what we had already derived from looking at the raw dissimilarity matrices! However, it is more intuitive and quicker to interpret than the second-order dissimilarity matrix plotted above. Nevertheless, bear in mind that MDS is a <strong>dimensionality reduction technique</strong> in the sense that it attempts to represent high-dimensional data in a lower-dimensional space. When there are many points to embed in a 2-D space, the pairwise distances represented by the 2-D plot <strong>can become distorted</strong>. For that reason, it is always a good idea to include the raw data alongside an MDS plot when presenting the results of your analysis.</p>
<p>You have made it to the <strong>end of this notebook</strong>, congratulations! If you found this notebook helpful, <strong>please consider leaving a star</strong> 🌟 <strong>on <a class="reference external" href="https://github.com/mrvnthss/intro-to-rsa">GitHub</a></strong>.</p>
</section>
</section>
<section id="additional-material">
<h2>Additional Material<a class="headerlink" href="#additional-material" title="Link to this heading">#</a></h2>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h3>
<p>These exercises are designed to deepen your understanding of representational similarity analysis. While it’s great to work through code, there’s no better way to learn than by writing your own code. While the exercises are numbered, there is absolutely no need to follow this order when working through them. In particular, any exercise can be completed without completing any other exercise, so you can pick and choose whichever exercise sounds most interesting to you.</p>
<ol class="arabic simple">
<li><p>Try to use a different subset of images of the CIFAR10 dataset for the analysis. Do the results change? What happens if you use more/fewer images per category?</p></li>
<li><p>Use images from a different dataset such as <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR100</a> or <a class="reference external" href="https://ieeexplore.ieee.org/document/5206848">ImageNet</a>.</p></li>
<li><p>Include some <a class="reference external" href="https://pytorch.org/vision/stable/models.html#classification">more networks</a> into the analysis!</p></li>
<li><p>Extract activations from additional layers other than the penultimate layer. <a class="reference external" href="https://pytorch.org/vision/main/generated/torchvision.models.feature_extraction.create_feature_extractor.html">Here</a> is the documentation of the <code class="docutils literal notranslate"><span class="pre">create_feature_extractor</span></code> method that we used to extract the activations. To see the names of the layers that you can extract from, simply print the network (e.g., <code class="docutils literal notranslate"><span class="pre">print(alexnet)</span></code>).</p></li>
<li><p>Try to replace Euclidean distance by another distance measure when computing RDMs in <em>Step 1</em> (e.g., <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.correlation.html">correlation distance</a>, <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html">cosine distance</a>). Can you find a way to implement these distances without the need for a for-loop? Does the representational similarity analysis yield different results when using the new distance measure?</p></li>
<li><p>Similarly, try replacing the rank correlation distance in <em>Step 2</em> of the analysis by a different dissimilarity measure. Again, how do the results compare to the ones obtained in this tutorial?</p></li>
<li><p>We used multidimensional scaling (MDS) to visually represent the second-order dissimilarity matrix of our analysis. In the same manner, we could visualize each of the (first-order) dissimilarity matrices that we obtained for the four networks included in our analysis. Try to do so! Also, what information does such a plot contain?</p></li>
</ol>
</section>
<section id="literature">
<h3>Literature<a class="headerlink" href="#literature" title="Link to this heading">#</a></h3>
<section id="foundations-reviews">
<h4>Foundations &amp; Reviews<a class="headerlink" href="#foundations-reviews" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://doi.org/10.3389/neuro.06.004.2008">Representational similarity analysis - Connecting the branches of systems neuroscience</a> (Kriegeskorte et al., 2008)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1016/j.tics.2013.06.007">Representational geometry: Integrating cognition, computation, and the brain</a> (Kriegeskorte and Kievit, 2013)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1146/annurev-neuro-080317-061906">Peeling the onion of brain representations</a> (Kriegeskorte and Diedrichsen, 2019)</p></li>
</ul>
</section>
<section id="statistical-methods-inference">
<h4>Statistical Methods &amp; Inference<a class="headerlink" href="#statistical-methods-inference" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2015.12.012">Reliability of dissimilarity measures for multi-voxel pattern analysis</a> (Walther et al., 2016)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1371/journal.pcbi.1006299">Representational structure or task structure? Bias in neural representational similarity analysis and a Bayesian method for reducing bias</a> (Cai et al., 2019)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.51628/001c.27664">Comparing representational geometries using whitened unbiased-distance-matrix similarity</a> (Diedrichsen et al., 2021)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.7554/eLife.82566">Statistical inference on representational geometries</a> (Schütt et al., 2023)</p></li>
</ul>
</section>
<section id="applications-challenges">
<h4>Applications &amp; Challenges<a class="headerlink" href="#applications-challenges" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://papers.nips.cc/paper_files/paper/2013/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf">Hierarchical modular optimization of convolutional networks achieves representations similar to macaque IT and human ventral stream</a> (Yamins et al., 2013)</p></li>
<li><p><a class="reference external" href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1003963&amp;amp;type=printable">Deep neural networks rival the representation of primate IT cortex for core visual object recognition</a> (Cadieu et al., 2014)</p></li>
<li><p><a class="reference external" href="https://www.pnas.org/doi/full/10.1073/pnas.1403112111">Performance-optimized hierarchical models predict neural responses in higher visual cortex</a> (Yamins et al., 2014)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1098/rstb.2016.0278">Inferring brain-computational mechanisms with models of activity measurements</a> (Kriegeskorte and Diedrichsen, 2016)</p></li>
<li><p><a class="reference external" href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005508&amp;amp;type=printable">Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</a> (Diedrichsen and Kriegeskorte, 2017)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1038/s41467-020-19632-w">Individual differences among deep neural network models</a> (Mehrer et al., 2020)</p></li>
<li><p><a class="reference external" href="https://pdf.sciencedirectassets.com/272195/1-s2.0-S0896627319X00227/1-s2.0-S089662732030605X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAYaCXVzLWVhc3QtMSJIMEYCIQCCIhzOuIvyR2Ij2ljKQogsRohLUh0ywlzonLDxyfgc4AIhAOuaEBFuxLZ5CmwvH471gBTmLrmWiLT8O9Iy0c8NczJbKrwFCL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igx322l7KNqGg5WuccAqkAWvibt6G3VPmHwY%2B6R2vNKEq2nlCjk8nEiOY2Em1bw%2BPNVgSEA64jBpk5vQ5S448oKF6yZuEr82unVuN3OXTHbwUz13fknjj%2Ff%2BIu93kqakb9phNiBqAoEuu1De%2FFG8FKMAlEBedLttT%2BpNWpbKR06tlgAPEszbXL0GVJ4fBEVTkowQebODRMCthMXTjihSOvGBH3aumwMaSdLjWwV7SGZG1UM9V6GEIIrJ6bLzN0AZgCc5nPpJSV2pNa23CTyJUP%2FQuTePJSynSyP4ftzuOlAhkk8bKxZSHtmS4Sj8t10DLRnr6NzeIoCPxXmbUQvntEcr%2ByDSjCqtYoOHmfLEkvjYGEwur0%2F7kpGX1cBJ7Fr%2BZP2JFEndKVp9YArR0uC4NCh8a34CR68meuNAgcK210rEpB91SF3H130qH4UM74mbOXzPSX0FmbD2%2B5P1Fb2rOFEg4OBrvmFpwn3bluF4qLJaYYOKILWX9hic7thnuKhZ65S9gV77FU330ndksiQQbCBsc%2B5zpG35vdvoTD%2FtNMpWqB0Sg6Bqr1rT%2FJgDtxYu9U0DjPMwxLINrLpKEk%2Bb31LoGtL65xMa6T%2FKGF2GUfM6a928Kak2MjVNgjay%2B2CaIeJR6j%2BnPXfLwhaQ8n6FWXbKyIq%2BWD4%2FNlALBtvoEOicjUdO34SjUbkamQtqUq%2FQ7zvZJ9M%2B%2FNP6N3eC6BjexyPnGqFQHZiNzQuHdU8VTHCudbMyRZCq9%2BZbyzYM0Z5U7Hbi9yT%2BvJcinIEo3k7WRc4H5HMTe%2BNz2Mvp2Sj4ojrbVMiFI0YH5gjuvGkDtOW%2BLvr40fFXvNTd%2B9VY3Q8vWD3VdYAA2IMgwHBCb35X6j2rGkmjc2gGk2rJoNqwd%2Fz4WDDH0uu6BjqwAeyojJAShu3IWRejKYQFTs%2BISN9uUYSWPSRcAYr79AB4U%2Fh2uCpl0DFvaaHd3MCf2%2BCVz31uO2nJ9rbuNGXVdUmxuGbq3iWw%2BvqDCRuaNfuZA80Zgd3lpkMHJDjMwrxfBYUPlAjL60GRoQsrH0bY12MIPlXHqBEUYlk%2B6vXa9tuqBshqqydnfvAJrHIUPR6b9ubAleMA%2F4RMd3JEB7EviZzKJRb2KoLtDeTpo3JgMBCW&amp;amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Date=20241212T142234Z&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Credential=ASIAQ3PHCVTY7OZDCS4B%2F20241212%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Signature=25d6790b8ebdc535f62867b66f69a7b9c76dab03e697bb93b0ff53e15c3b3fb0&amp;amp;hash=3c5b4501b3c9936d02458f1cba10a652e75d52074a80a01b5a670b3982a6a517&amp;amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;amp;pii=S089662732030605X&amp;amp;tid=spdf-eb53c098-db70-4a70-b48b-af703aae00d7&amp;amp;sid=993d324137eb864b3209a6e078cbec1973degxrqb&amp;amp;type=client&amp;amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;amp;ua=1d045909500156055357&amp;amp;rr=8f0e5e06ef02ef45&amp;amp;cc=gb">Integrative benchmarking to advance neurally mechanistic models of human intelligence</a> (Schrimpf et al., 2020)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1101/2022.04.05.487135">Obstacles to inferring mechanistic similarity using representational similarity analysis</a> (Dujmović et al., 2023)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1101/2024.08.07.607035">Conclusions about neural network to brain alignment are profoundly impacted by the similarity measure</a> (Soni et al., 2024)</p></li>
<li><p><a class="reference external" href="https://doi.org/10.48550/arXiv.2310.13018">Getting aligned on representational alignment</a> (Sucholutsky et al., 2024)</p></li>
</ul>
</section>
<section id="tools-libraries-tutorials">
<h4>Tools, Libraries &amp; Tutorials<a class="headerlink" href="#tools-libraries-tutorials" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://doi.org/10.1371/journal.pcbi.1003553">A toolbox for representational similarity analysis</a> (Nili et al., 2014)</p></li>
<li><p><a class="reference external" href="https://github.com/rsagroup/rsatoolbox">rsatoolbox</a> (Python library for RSA)</p></li>
<li><p><a class="reference external" href="https://cbmm.mit.edu/video/tutorial-statistical-inference-representational-geometries">Statistical inference on representational geometries</a> (Advanced tutorial on performing statistical inference using RSA)</p></li>
</ul>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="linear_classifier_probe.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">7.3. Probing by linear classifiers</p>
      </div>
    </a>
    <a class="right-next"
       href="big_projects.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">8. Big Projects</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-rsa">Why RSA?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-preparations">Prerequisites &amp; Preparations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-device">Compute Device</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-networks">Downloading Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cifar10">CIFAR10</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-comparing-network-representations">Case Study: Comparing Network Representations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">Outline</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-0-collecting-data">Step 0: Collecting Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-computing-pairwise-dissimilarities">Step 1: Computing Pairwise Dissimilarities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-comparing-representations">Step 2: Comparing Representations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-material">Additional Material</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#literature">Literature</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#foundations-reviews">Foundations &amp; Reviews</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-methods-inference">Statistical Methods &amp; Inference</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-challenges">Applications &amp; Challenges</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-libraries-tutorials">Tools, Libraries &amp; Tutorials</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Arash Akbarinia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>