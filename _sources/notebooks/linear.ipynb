{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b64b5fe-add8-4867-baa1-533c352cfacb",
   "metadata": {},
   "source": [
    "# 1.4. Linear Layer\n",
    "\n",
    "A **linear layer** (also known as a **fully connected** or **dense** layer) applies an affine transformation to the input data, transforming it linearly. This transformation can be represented mathematically as:\n",
    "\n",
    "$\n",
    "y = xA^T + b\n",
    "$\n",
    "\n",
    "where:\n",
    "- $x$ is the input,\n",
    "- $A$ represents the layer’s weights,\n",
    "- and $b$ is the bias term.\n",
    "\n",
    "The linear layer is a foundational component in neural networks and is often used to map higher-dimensional data into a lower-dimensional space, such as for final classification outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b042394-4e79-4a5e-bdc3-d98452877bcc",
   "metadata": {},
   "source": [
    "## Convolutional Architectures\n",
    "\n",
    "In traditional CNN architectures, such as AlexNet and VGG, linear layers are typically found at the end of the network. After a series of convolutional layers, which extract features from the input data, the network often includes a few fully connected layers to interpret these features and make predictions. In AlexNet and VGG, the last three layers are fully connected and are responsible for the final classification.\n",
    "\n",
    "In more recent architectures like **ResNet**, linear layers are less prominent, with only the final classification layer being linear. This final layer maps the network’s feature space to the required output nodes—e.g., for ImageNet, the linear layer provides outputs for 1,000 different classes. This makes the network more efficient by focusing most of the computation on convolutional layers, while the linear layer simply provides the final output mapping.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea39ace-1a3e-414c-aea3-baf085906aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([128, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary components from PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define a linear layer with 2048 input features and 1000 output features\n",
    "# This example simulates a final classification layer mapping 2048 features to 1000 classes\n",
    "linear_layer = nn.Linear(2048, 1000)\n",
    "\n",
    "# Create a simulated input tensor of size (128, 2048), where 128 represents batch size\n",
    "# and 2048 represents the feature vector length for each input\n",
    "simulated_input = torch.randn(128, 2048)\n",
    "\n",
    "# Apply the linear transformation to the simulated input\n",
    "output = linear_layer(simulated_input)\n",
    "\n",
    "# Print the output shape to verify the transformation result\n",
    "print(\"Output size:\", output.size())  # Expected shape: (128, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680583a1-47bc-4dce-9476-a0cf20ede38c",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- We define a linear layer with **2,048 input features** and **1,000 output features**, which could represent a typical setup for a classification task on a dataset with 1,000 classes.\n",
    "- We simulate an input batch of size 128, where each item has 2,048 features.\n",
    "- The layer transforms this input into an output of shape (128, 1,000), showing how each input is mapped to 1,000 possible output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ca686-668d-4caf-a33a-eb40b836d625",
   "metadata": {},
   "source": [
    "## Transformer Architectures\n",
    "\n",
    "Transformer models, such as BERT and Vision Transformers (ViTs), make extensive use of **linear layers** within their architecture, especially within the **Multilayer Perceptron (MLP)** components. These linear layers are integral in projecting features to higher or lower dimensions, which supports tasks like attention mechanisms, information integration, and classification. We'll explore these aspects in greater depth in the upcoming chapter focused on transformer architectures.\n",
    "\n",
    "For now, let's walk through a simple example to see how an MLP can be implemented in PyTorch. In this example, we’ll use dimensions similar to those in the ViT architecture, where inputs of size 768 are mapped to 3072 dimensions in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6bdd18-e5ce-4dbd-b719-e961926c3ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define the Multilayer Perceptron (MLP) class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define a sequential layer stack\n",
    "        # This architecture follows the ViT (Vision Transformer) structure with:\n",
    "        # - An input layer mapping 768 nodes to a hidden layer with 3072 nodes\n",
    "        # - A ReLU activation function introducing non-linearity\n",
    "        # - An output layer that returns to the original 768 nodes\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(768, 3072, bias=True),  # Linear layer with bias term\n",
    "            nn.ReLU(),                       # ReLU activation to introduce non-linearity\n",
    "            nn.Linear(3072, 768, bias=True)  # Linear layer returning to 768 dimensions\n",
    "        )\n",
    "\n",
    "    # Define the forward pass of the MLP\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)  # Apply the sequential layers to input x\n",
    "\n",
    "# Instantiate the MLP model\n",
    "MLP_model = MLP()\n",
    "print(MLP_model)  # Display the model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512acbe-9a76-4f9c-9ddf-eafb5101135c",
   "metadata": {},
   "source": [
    "In this **MLP** example:\n",
    "- The first linear layer expands the input from 768 dimensions to 3072 dimensions.\n",
    "- The **ReLU activation** introduces a non-linearity, allowing the network to model more complex relationships.\n",
    "- The final layer reduces the dimensionality back to 768, ensuring compatibility with the input’s original size if needed for later processing in the transformer model.\n",
    "\n",
    "This basic MLP structure demonstrates how transformers manage feature transformations within their architectures. We'll explore transformers and their role in detail in later sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dde00a-30ea-4fd2-8ee0-a347cf8815e0",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The remarkable success of artificial neural networks today can be traced back to foundational ideas from the early days of computational modelling. This journey began with the **perceptron**, developed by [Frank Rosenblatt in 1957](https://bpb-us-e2.wpmucdn.com/websites.umass.edu/dist/a/27637/files/2016/03/rosenblatt-1957.pdf). Rosenblatt's perceptron was one of the first models that could learn to distinguish patterns through a simple linear transformation. This development represented a breakthrough by introducing the concept of a learning algorithm based on data.\n",
    "\n",
    "The perceptron's limitations, however, restricted it to solving only linearly separable problems. This led to further innovations in neural modelling, including the **multilayer perceptron (MLP)**, proposed by Ivakhnenko and Lapa. The MLP uses multiple layers of perceptrons with non-linear activation functions, allowing networks to solve more complex, non-linear problems by learning intricate patterns in data.\n",
    "\n",
    "For an in-depth historical perspective, we recommend reading [The Road to Modern AI](https://arxiv.org/pdf/2212.11279), which covers the milestones that have shaped artificial intelligence into what we see today. This document highlights how each step in neural network development, from Rosenblatt’s perceptron to contemporary deep learning models, has contributed to our current capabilities in AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
