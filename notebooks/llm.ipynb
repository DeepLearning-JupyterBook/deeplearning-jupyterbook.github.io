{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# 6.4. Large Language Model\n",
    "\n",
    "Since the introduction of ChatGPT in 2022, large language models (LLMs) have gained significant public attention as a prominent application of artificial intelligence. In this tutorial, we will delve into the underlying operations of modern LLMs, and we'll implement a basic language model that can generate text based on a small dataset.\n",
    "\n",
    "At a fundamental level, language models can be pretrained to predict either the continuation of a text segment or fill in missing parts of the segment. These two main types are:\n",
    "\n",
    "- **Autoregressive models:** Predict the continuation of a segment. For example, given the prompt \"I like to eat,\" the model might predict \"pizza\" or \"ice cream.\"\n",
    "- **Masked models (also called 'cloze' models):** Fill in the missing parts of a segment. For example, given \"I like to `[__]` `[__]` cream,\" the model might predict that \"eat\" and \"ice\" are the missing words.\n",
    "\n",
    "In this notebook, we will implement the autoregressive approach. This means that our model will focus on predicting the next \"token\" (or smallest unit of meaning) in a sequence, based on prior context.\n",
    "\n",
    "LLMs are statistical models. They learn, from large sets of internet text, the probability of the next token given the preceding context. This training is **self-supervised**, meaning they learn from the data itself without needing labelled answers.\n",
    "\n",
    "The presented materials here are inspired by Andrej Karpathy's excellent work on [NanoGPT](https://github.com/karpathy/nanoGPT), and I highly recommend his [lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy) on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "\n",
    "Let’s start by importing the necessary libraries and setting a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x762f3a8c4c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "torch.manual_seed(1365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "We start with loading the dataset to understand what our model will train on. Here, we’ll use the \"Tiny Shakespeare\" dataset, which compiles Shakespeare’s works into a single text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5hjCcLDr2WC",
    "outputId": "ccc60f0c-fd78-4dbe-8598-0512d1036aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-11 08:31:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘data/tinyshakespeare.txt’\n",
      "\n",
      "data/tinyshakespear 100%[===================>]   1,06M  --.-KB/s    in 0,05s   \n",
      "\n",
      "2024-11-11 08:31:15 (22,6 MB/s) - ‘data/tinyshakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a folder for the data, if it doesn't already exist\n",
    "os.makedirs(\"data/\", exist_ok=True)\n",
    "\n",
    "# Download the Tiny Shakespeare dataset into our data folder\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O data/tinyshakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# Read and display the dataset\n",
    "with open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    db_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters in the dataset: {len(db_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tiny Shakespeare dataset has over 1.1 million characters. Let’s print the first 250 characters to get a sense of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 250 characters\n",
    "print(db_text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "Our language model works by predicting the next **token** based on the preceding context. A **token** is simply a unit of text that the model understands and processes. In this tutorial, we’ll use individual characters as tokens to keep things straightforward. This means that the model will learn to predict the next character based on previous characters in the sequence.\n",
    "\n",
    "In larger language models, tokens can represent more complex units, such as entire words or subwords. Word-level tokenisation can be useful for capturing meaning in longer pieces of text. For example, a phrase like \"natural language processing\" might be divided into words as tokens, rather than individual letters. You can read more about this in the [text-classification](text_classification.ipynb) notebook.\n",
    "\n",
    "In this project, however, we’ll keep it simple and focus on character-level tokens. This approach allows us to train a smaller model while still learning basic patterns and sequences within text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, let’s identify all unique characters in our dataset and assign each one an integer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters in the dataset: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Total number of unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "# Unique characters in the text\n",
    "db_chars = sorted(list(set(db_text)))\n",
    "vocab_size = len(db_chars)\n",
    "print(\"Unique characters in the dataset:\", ''.join(db_chars))\n",
    "print(\"Total number of unique characters:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniser\n",
    "\n",
    "Computers only understand numbers. For instance, in images, colours are represented by numbers (e.g., 0 represents black, and 255 represents white). Text data follows the same principle: characters need to be converted into numbers that the model can work with. Therefore, we’ll convert each character into a unique integer. This is essential because our model operates on numerical data, not raw text.\n",
    "\n",
    "To do this, we’ll create a **tokeniser** that assigns a unique integer to each character in the dataset. We achieve this by iterating through all characters, assigning an integer to each one in the order they appear in the dataset. This mapping is implemented in the `str2int` dictionary, which maps characters to integers, and `int2str`, which maps integers back to characters.\n",
    "\n",
    "Next, we define two `lambda` functions—`encode_txt` and `decode_txt`—to handle conversions between lists of characters and their integer representations. The `encode_txt` function takes a string and returns a list of integers, while `decode_txt` does the reverse, converting a list of integers back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Map characters to integers\n",
    "str2int = {ch: i for i, ch in enumerate(db_chars)}\n",
    "int2str = {i: ch for i, ch in enumerate(db_chars)}\n",
    "\n",
    "# Functions for encoding and decoding\n",
    "encode_txt = lambda s: [str2int[c] for c in s]  # Encode: converts string to list of integers\n",
    "decode_txt = lambda l: ''.join([int2str[i] for i in l])  # Decode: converts list of integers to string\n",
    "\n",
    "print(encode_txt(\"hii there\"))\n",
    "print(decode_txt(encode_txt(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in our mapping that each character has a unique numerical code. For example, `h` is mapped to `46`, `i` to `47`, and the space (` `) to `1`. To illustrate how our language model views the data, let’s print the first 250 characters as numbers to see how it would appear to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53, 1, 42, 47, 43, 1, 58, 46, 39, 52, 1, 58, 53, 1, 44, 39, 51, 47, 57, 46, 12, 0, 0, 13, 50, 50, 10, 0, 30, 43, 57, 53, 50, 60, 43, 42, 8, 1, 56, 43, 57, 53, 50, 60, 43, 42, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 18, 47, 56, 57, 58, 6, 1, 63, 53, 59, 1, 49, 52, 53, 61, 1, 15, 39, 47, 59, 57, 1, 25, 39, 56, 41, 47, 59, 57, 1, 47, 57, 1, 41, 46, 47, 43, 44, 1, 43, 52, 43, 51, 63, 1, 58, 53, 1, 58, 46, 43, 1, 54, 43, 53, 54, 50, 43, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "print(encode_txt(db_text[:250]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original text, you may have noticed that there are line breaks between different parts of the conversation (for example, between dialogue by \"first-citizen\" and \"All\"). How are these line breaks represented numerically? We can check the encoding for the line break symbol `\\n` in our tokeniser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_txt('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our list of numbers above, the number `0` represents a line break. This allows the language model to understand structural elements in the text, such as new lines, even though it processes everything as numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset Preparation\n",
    "\n",
    "Before training a language model in PyTorch, we need to prepare our text data by converting it into a format that PyTorch can process. Specifically, we’ll convert our dataset to a PyTorch `torch.Tensor`, which will hold the data as numerical values that the model can use.\n",
    "\n",
    "1. **Convert text to a tensor**: Using our earlier tokenisation, we’ll encode the entire text dataset into numbers and store it as a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Encode the text as integers and store in a tensor\n",
    "db_tensor = torch.tensor(encode_txt(db_text), dtype=torch.long)\n",
    "\n",
    "# Print the shape and data type of the tensor to confirm\n",
    "print(db_tensor.shape, db_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Split data into training and validation sets**: To train effectively, we need a training set (90% of the data) and a validation set (10% of the data) to monitor the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Split the tensor into training (first 90%) and validation (last 10%) sets\n",
    "n = int(0.9 * len(db_tensor))  \n",
    "train_data = db_tensor[:n]\n",
    "val_data = db_tensor[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Model\n",
    "\n",
    "An **autoregressive model** predicts each character based on the sequence of preceding characters. This setup means that, for each step, the model uses the characters it has already seen to predict the next one. Let’s walk through how this works.\n",
    "\n",
    "1. **Define a context length (block size)**: The context length (or **block size**) defines how many previous characters the model considers to predict the next character. Here, we set `block_size` to 12, meaning the model will look at the past 12 characters when making each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM sees: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52])\n",
      "Human sees: First Citizen\n"
     ]
    }
   ],
   "source": [
    "block_size = 12\n",
    "print(f\"LLM sees: {train_data[:block_size+1]}\")\n",
    "print(f\"Human sees: {decode_txt(train_data[:block_size+1].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Autoregressive training example**: During training, the model’s goal is to predict the next character based on the sequence it has just seen. For instance, if the model sees the character \"F\" (encoded as `18`), it should predict the next character, \"i\" (encoded as `47`).\n",
    "\n",
    "Let’s print each input character and its expected target character across our chosen block of 12 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([18])                                                 target: 47\n",
      "input: tensor([18, 47])                                             target: 56\n",
      "input: tensor([18, 47, 56])                                         target: 57\n",
      "input: tensor([18, 47, 56, 57])                                     target: 58\n",
      "input: tensor([18, 47, 56, 57, 58])                                 target: 1\n",
      "input: tensor([18, 47, 56, 57, 58,  1])                             target: 15\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15])                         target: 47\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47])                     target: 58\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])                 target: 47\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])             target: 64\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64])         target: 43\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43])     target: 52\n"
     ]
    }
   ],
   "source": [
    "# Define input and target sequences for our block size\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "max_width = block_size * 5  # Set max width for aligned printing\n",
    "\n",
    "# Display the input and target character-by-character\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    context_str = f\"{context}\".ljust(max_width)  # Left-justify with padding\n",
    "    print(f\"input: {context_str} target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Human-readable format**: To make the output clearer, let’s print the characters in their original text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: F            target: i\n",
      "input: Fi           target: r\n",
      "input: Fir          target: s\n",
      "input: Firs         target: t\n",
      "input: First        target:  \n",
      "input: First        target: C\n",
      "input: First C      target: i\n",
      "input: First Ci     target: t\n",
      "input: First Cit    target: i\n",
      "input: First Citi   target: z\n",
      "input: First Citiz  target: e\n",
      "input: First Citize target: n\n"
     ]
    }
   ],
   "source": [
    "# Display in human-readable form\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "max_width = block_size  # Set width for readability\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    context_str = f\"{decode_txt(context.tolist())}\".ljust(max_width)  # Left-justify with padding\n",
    "    print(f\"input: {context_str} target: {int2str[target.tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a typical approach to handle batching is through a `Dataset` class (inheriting from `torch.utils.data.Dataset`) and `DataLoader` (from `torch.utils.data.DataLoader`). However, in this simple setup, we’ll use a function called `get_batch` to generate batches of data for us.\n",
    "\n",
    "The `get_batch` function works as follows:\n",
    "\n",
    "- It takes a split (`'train'` or `'val'`) and selects either the training or validation dataset.\n",
    "- It then randomly selects starting indices for sequences of length `block_size` and extracts corresponding input (`x`) and target (`y`) sequences.\n",
    "- `x` represents the input batch, and `y` represents the target batch, shifted by one token, meaning that each `x[i]` sequence predicts the following `y[i]` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, device='cpu'):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set up a batch to see its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input data in the batch: torch.Size([4, 12])\n",
      "Size of target in the batch: torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # number of sequences processed in parallel\n",
    "block_size = 12 # maximum context length for predictions\n",
    "\n",
    "batch_data, target_tokens = get_batch('train')\n",
    "print(f\"Size of input data in the batch: {batch_data.shape}\")\n",
    "print(f\"Size of target in the batch: {target_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network\n",
    "\n",
    "We’ll implement two types of models:\n",
    "\n",
    "1. **Bigram Language Model**: A simple n-gram model that considers only the previous token for prediction.\n",
    "2. **GPT-based Language Model**: A more complex transformer-based model similar to those used in state-of-the-art LLMs like ChatGPT and Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model\n",
    "\n",
    "A **bigram language model** is an example of an **n-gram model** where `n = 2`, meaning it considers only one previous token to predict the next one. If we considered two tokens, we would have a **trigram model**. This type of model learns dependencies based on pairs of tokens.\n",
    "\n",
    "The `BigramLanguageModel` class is built with:\n",
    "\n",
    "1. **Initialising the Model**:\n",
    "   - The model is a subclass of `nn.Module`, which provides essential methods to manage model behaviour.\n",
    "   - `token_embedding_table`: This `nn.Embedding` layer maps each token in our vocabulary to a vector of values that the model can optimise during training. Here, we’re using a square matrix of size `vocab_size x vocab_size`, which lets the model learn relationships between each pair of tokens directly similar to a lookup table.\n",
    "\n",
    "2. **Forward Pass** (`forward` method):\n",
    "   - This method takes `input_tokens`, which is a batch of sequences of token indices, and passes them through the embedding layer to get `logits`. Here, `logits` represent the raw output scores for each token, before applying softmax.\n",
    "\n",
    "3. **Calculating the Loss** (`calculate_loss` method):\n",
    "   - The model uses the `cross_entropy` loss function to calculate **negative log-likelihood**, which measures how well the model’s predictions (logits) match the target tokens.\n",
    "   - To use cross-entropy, we flatten the `logits` and `target_tokens` tensors into two-dimensional arrays. This allows the function to calculate the loss across all tokens in the batch at once.\n",
    "   - The `staticmethod` decorator indicates that a method is associated with the class itself, rather than any particular instance of the class. This is useful if, for instance, we want to have a standardized way of computing loss that applies universally across all instances of the model or if we want to call the loss calculation outside the class instance.\n",
    "\n",
    "4. **Generating a Sequence** (`generate_sequence` method):\n",
    "   - The `generate_sequence` method starts with a sequence (such as a single token) and generates new tokens one by one.\n",
    "   - In each step, it:\n",
    "     - Passes the input sequence through the model.\n",
    "     - Focuses only on the logits for the most recent token (last in sequence).\n",
    "     - Applies a `softmax` function to convert logits to probabilities.\n",
    "     - Uses `torch.multinomial` to sample the next token based on the probabilities, which introduces an element of randomness.\n",
    "     - Adds the new token to the sequence and repeats until reaching the maximum specified length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()  # Initialise parent class (nn.Module)\n",
    "        # Embedding table that maps each token to an embedding vector\n",
    "        # This table is of size (vocab_size, vocab_size) so each token can \"read\" the logits of the next token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        \"\"\"\n",
    "        Forward pass: This function processes input data through the model.\n",
    "        \n",
    "        Args:\n",
    "        input_tokens (Tensor): Batch of input sequences with shape (batch_size, sequence_length).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output logits with shape (batch_size, sequence_length, vocab_size).\n",
    "        \"\"\"\n",
    "        # Pass input tokens through embedding layer\n",
    "        logits = self.token_embedding_table(input_tokens)  # (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(logits, target_tokens):\n",
    "        \"\"\"\n",
    "        Calculate cross-entropy loss comparing predicted logits to actual target tokens.\n",
    "        \n",
    "        Args:\n",
    "        logits (Tensor): Model output logits of shape (batch_size, sequence_length, vocab_size).\n",
    "        target_tokens (Tensor): Ground truth tokens of shape (batch_size, sequence_length).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Calculated cross-entropy loss.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, vocab_size = logits.shape  # Unpack tensor dimensions\n",
    "        logits = logits.view(batch_size * sequence_length, vocab_size)  # Flatten for cross-entropy\n",
    "        target_tokens = target_tokens.view(batch_size * sequence_length)  # Flatten for cross-entropy\n",
    "        loss = F.cross_entropy(logits, target_tokens)  # Calculate cross-entropy loss\n",
    "        return loss\n",
    "\n",
    "    def generate_sequence(self, input_tokens, max_length):\n",
    "        \"\"\"\n",
    "        Generate a sequence by predicting one token at a time based on previous tokens.\n",
    "        \n",
    "        Args:\n",
    "        input_tokens (Tensor): Initial token to start generating from, of shape (batch_size, 1).\n",
    "        max_length (int): Number of new tokens to generate.\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Generated token sequence.\n",
    "        \"\"\"\n",
    "        for _ in range(max_length):\n",
    "            # Run the input through the model to get logits (predictions)\n",
    "            logits = self.forward(input_tokens)\n",
    "            # Get only the logits for the last token position\n",
    "            logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Randomly select the next token based on the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)  # (batch_size, 1)\n",
    "            # Add the new token to the input tokens sequence\n",
    "            input_tokens = torch.cat((input_tokens, next_token), dim=1)  # (batch_size, sequence_length + 1)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of `BigramLanguageModel` and check its structure. The `Embedding` matrix has dimensions of 65 by 65, reflecting the number of unique characters in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 65)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "bigram_net = BigramLanguageModel(vocab_size)\n",
    "print(bigram_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s input one batch of data into the model to observe its behaviour. Here, we can check the shape of the output, which should have three dimensions: `(batch_size, sequence_length, vocab_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output size: torch.Size([4, 12, 65])\n"
     ]
    }
   ],
   "source": [
    "output_logits = bigram_net(batch_data)\n",
    "print(f\"Model's output size: {output_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the initial loss to see how well (or poorly) the untrained model performs. For comparison, we can calculate a \"chance-level\" baseline loss, which would represent a model that predicts tokens randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (untrained model): 4.630\n",
      "Chance-level baseline loss: 4.174\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the loss\n",
    "initial_loss = bigram_net.calculate_loss(output_logits, target_tokens)\n",
    "print(f\"Initial loss (untrained model): {initial_loss:.3f}\")\n",
    "\n",
    "# Calculate the chance-level baseline\n",
    "chance_level_loss = -np.log(1 / vocab_size)\n",
    "print(f\"Chance-level baseline loss: {chance_level_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s use our untrained model to generate a sequence of tokens. We expect the output to be somewhat random, as the model hasn’t learned any patterns yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QwuiVZZEKzSKlV.ATrRlzEaV?3ZBWApyiBkQdtNAz\n",
      "uqMVCD.jNMGgDmC&OuoDLYpVu\n",
      "uMTClnrk,AaIagaUx 'zkl,ATe\n",
      "?csZ&\n"
     ]
    }
   ],
   "source": [
    "# Generate a text sequence from the initial token\n",
    "start_token = torch.zeros((1, 1), dtype=torch.long)  # Start with token \"0\"\n",
    "generated_sequence = bigram_net.generate_sequence(start_token, max_length=100)\n",
    "\n",
    "# Convert generated token indices back to characters\n",
    "generated_text = decode_txt(generated_sequence[0].tolist())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will show a random sequence of characters, which is expected at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Architecture and Self-Attention\n",
    "\n",
    "To create a modern language model like GPT, we need to use a **transformer architecture**. Introduced in 2017 in the influential paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), transformers have become a versatile architecture applied across various types of data, from text to images and beyond. The key operation in transformers is **self-attention**, which is referred to as \"Scaled Dot-Product Attention\" in the original paper, defines as:\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$$\n",
    "where $Q$, $K$, and $V$ denote the query, key and value, respectively, and $d_k$ is the dimensionality of keys.\n",
    "\n",
    "**Attention** can be thought of as a communication system where tokens in a sequence “look” at each other and learn to focus on certain tokens based on relevance. Imagine each token as a node in a directed graph, where each node collects information from the others, weighted by how important each connection is. This weighted communication allows the model to learn relationships between tokens in a sequence, forming the basis of powerful language models.\n",
    "\n",
    "The following code implements key components of the transformer architecture: `SelfAttention`, `MultiHeadAttention`, `MLP`, and `TransformerBlock`. Let’s go through each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention\n",
    "\n",
    "In transformers, **self-attention** enables each token to focus on other tokens in the sequence, calculating a unique attention weight for each possible pair of tokens. In the code below, we define a `SelfAttention` class that represents a single \"head\" of self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"One head of self-attention: calculates attention for each token in relation to others.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # Linear transformations for computing the key, query, and value matrices\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        \n",
    "        # Create a lower-triangular mask for future tokens (causal mask)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for self-attention head.\n",
    "        \n",
    "        Args:\n",
    "        x (Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor of shape (batch_size, sequence_length, head_size).\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, embedding_dim = x.shape\n",
    "        \n",
    "        # Calculate key, query, and value matrices\n",
    "        keys = self.key(x)    # Shape: (batch_size, sequence_length, head_size)\n",
    "        queries = self.query(x)  # Shape: (batch_size, sequence_length, head_size)\n",
    "        \n",
    "        # Compute attention scores by taking dot product of queries and keys\n",
    "        # Scaled by square root of head_size to maintain stable gradients\n",
    "        attention_scores = queries @ keys.transpose(-2, -1) * (embedding_dim ** -0.5)\n",
    "        \n",
    "        # Apply causal mask to prevent attention to future tokens\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        \n",
    "        # Convert attention scores to probabilities\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Calculate weighted sum of values\n",
    "        values = self.value(x)\n",
    "        output = attention_probs @ values  # Shape: (batch_size, sequence_length, head_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Key, Query, and Value matrices**: Each input token is transformed into these three representations. The dot product of `queries` and `keys` produces an attention score for each token pair.\n",
    "- **Causal Mask**: We use a triangular mask so each token can only attend to previous tokens, ensuring future information isn’t used when predicting the next token.\n",
    "- **Softmax and Weighted Sum**: We apply softmax to convert the scores to probabilities, which are then used to calculate a weighted sum of the `values`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "To enhance the model’s capacity to learn complex relationships, transformers use **multi-head attention**, which runs several self-attention heads in parallel. Each head learns different aspects of the relationships between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Combines multiple self-attention heads in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # Initialise multiple self-attention heads\n",
    "        self.heads = nn.ModuleList([SelfAttention(head_size, embedding_dim, dropout_rate) for _ in range(num_heads)])\n",
    "        # Project concatenated output of all heads back to embedding dimension\n",
    "        self.proj = nn.Linear(head_size * num_heads, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "        x (Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "        \"\"\"\n",
    "        # Concatenate outputs from each head along the last dimension\n",
    "        multi_head_output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        \n",
    "        # Apply final linear projection and dropout\n",
    "        output = self.dropout(self.proj(multi_head_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `MultiHeadAttention` combines the output of each self-attention head and applies a final projection layer to bring it back to the original embedding dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron (MLP)\n",
    "\n",
    "Following the attention layers, transformers apply a simple neural network called an **MLP (Multilayer Perceptron)**, which learns further transformations on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Defines a feedforward neural network (MLP) for additional processing after attention.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),  # Expand the embedding dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),  # Project back down\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the MLP.\n",
    "        \n",
    "        Args:\n",
    "        x (Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Processed tensor of the same shape.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MLP increases the dimensions temporarily before reducing them back, allowing for more complex transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Block\n",
    "\n",
    "Finally, we combine self-attention and MLP layers into a **Transformer Block**. This block is the core building unit of transformers, and each one includes both self-attention and feedforward (MLP) processing with layer normalisation applied to stabilise learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Defines a single transformer block with self-attention and MLP layers.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "        self.attention = MultiHeadAttention(num_heads, head_size, embedding_dim, dropout_rate)\n",
    "        self.feedforward = MLP(embedding_dim, dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block.\n",
    "        \n",
    "        Args:\n",
    "        x (Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor of the same shape.\n",
    "        \"\"\"\n",
    "        # Apply multi-head attention followed by layer normalisation\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        # Apply MLP followed by layer normalisation\n",
    "        x = x + self.feedforward(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block:\n",
    "- **Multi-Head Attention and MLP layers** provide the model with the ability to learn dependencies in different ways.\n",
    "- **Layer Normalisation** helps stabilise learning by scaling the data before each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Language Model\n",
    "\n",
    "Now that we’ve built the foundational components of the transformer architecture, we can create the `GPTLanguageModel`. This model is somewhat similar to a simpler `BigramLanguageModel`, with one major distinction: instead of directly predicting the next token based on bigram frequencies, `GPTLanguageModel` leverages the transformer architecture. It uses several layers of transformer blocks, each containing multiple self-attention heads followed by an MLP, enabling it to understand context over longer sequences.\n",
    "\n",
    "The following code defines `GPTLanguageModel`, which embeds tokens and positions, then passes them through the transformer layers before generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"A GPT-based language model that utilises transformer blocks to generate sequences.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=64, num_heads=4, num_layers=4, dropout_rate=0):\n",
    "        \"\"\"\n",
    "        Initialises the model with specified vocabulary size, embedding dimension, number of heads,\n",
    "        number of transformer layers, and dropout rate.\n",
    "\n",
    "        Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embedding_dim (int): Dimension of token embeddings.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        num_layers (int): Number of transformer layers.\n",
    "        dropout_rate (float): Dropout probability for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer for token representation\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Embedding layer for positional representation to add sequence information\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(embedding_dim, num_heads=num_heads, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # Final layer normalisation for stable outputs\n",
    "        self.final_layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Output layer mapping the final transformer output to vocabulary size\n",
    "        self.language_model_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # Initialise weights for stability\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialises weights for linear and embedding layers.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "        input_ids (Tensor): Tensor of shape (batch_size, sequence_length) with input token indices.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Logits of shape (batch_size, sequence_length, vocab_size) indicating probabilities of each token.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "        \n",
    "        # Create token embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Create positional embeddings to give a sense of order in the sequence\n",
    "        positions = torch.arange(sequence_length, device=input_ids.device)\n",
    "        position_embeddings = self.position_embedding(positions)  # Shape: (sequence_length, embedding_dim)\n",
    "        \n",
    "        # Add token and positional embeddings\n",
    "        x = token_embeddings + position_embeddings  # Combined shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Pass through stacked transformer blocks\n",
    "        x = self.transformer_blocks(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Apply final layer normalisation\n",
    "        x = self.final_layer_norm(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Convert to logits for each token in the vocabulary\n",
    "        logits = self.language_model_head(x)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(logits, target_ids):\n",
    "        \"\"\"\n",
    "        Calculates cross-entropy loss between predicted logits and target token indices.\n",
    "\n",
    "        Args:\n",
    "        logits (Tensor): Predicted logits of shape (batch_size, sequence_length, vocab_size).\n",
    "        target_ids (Tensor): Target indices of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Calculated loss value.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        logits = logits.view(batch_size * sequence_length, vocab_size)\n",
    "        target_ids = target_ids.view(batch_size * sequence_length)\n",
    "        \n",
    "        # Cross-entropy loss over flattened logits and target\n",
    "        loss = F.cross_entropy(logits, target_ids)\n",
    "        return loss\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates text by iteratively sampling new tokens.\n",
    "\n",
    "        Args:\n",
    "        input_ids (Tensor): Initial token indices of shape (batch_size, initial_sequence_length).\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Expanded sequence with newly generated tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Focus on last tokens within block size\n",
    "            input_ids_cond = input_ids[:, -block_size:]\n",
    "            \n",
    "            # Forward pass to get logits\n",
    "            logits = self.forward(input_ids_cond)\n",
    "            \n",
    "            # Focus only on the last token's logits\n",
    "            logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = F.softmax(logits, dim=-1)  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Sample from the probability distribution to get next token index\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Append sampled token to input_ids\n",
    "            input_ids = torch.cat((input_ids, next_token_id), dim=1)  # Updated shape: (batch_size, current_length + 1)\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go over each component in the model:\n",
    "\n",
    "- **Token and Positional Embeddings**: In language models, each token is represented as a vector using an embedding table. Since transformers do not inherently have a sense of order, we add positional embeddings to provide sequence information.\n",
    "  \n",
    "- **Transformer Blocks**: The model consists of multiple transformer blocks stacked together. Each block has multi-headed self-attention and feedforward MLP layers, enabling the model to focus on different parts of the input sequence.\n",
    "  \n",
    "- **Final Layer Norm and Language Model Head**: After passing through the transformer layers, we apply layer normalisation to stabilise the output. The language model head maps this output to logits representing the vocabulary.\n",
    "\n",
    "- **Weight Initialisation**: Initialising weights in specific layers stabilises training and enhances model performance.\n",
    "\n",
    "\n",
    "To understand the model’s structure, we can create an instance of `GPTLanguageModel` and print its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLanguageModel(\n",
      "  (token_embedding): Embedding(65, 64)\n",
      "  (position_embedding): Embedding(12, 64)\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (feedforward): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (3): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (feedforward): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (3): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (feedforward): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (3): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x SelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (feedforward): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (3): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (language_model_head): Linear(in_features=64, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create GPTLanguageModel instance\n",
    "gpt_net = GPTLanguageModel(vocab_size=vocab_size)\n",
    "\n",
    "# Print the model architecture\n",
    "print(gpt_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Now that we have both our dataset and model architecture set up, we are ready to train the network. Training involves optimising the model’s parameters so that it can make accurate predictions on unseen data. This section includes utility functions to monitor performance during training and the main training loop that optimises the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "The `estimate_loss` function is designed to evaluate the model's performance on both training and validation datasets. By monitoring these metrics periodically, we can gauge how well the model is learning and whether it is overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disables gradient calculations for evaluation\n",
    "def estimate_loss(model, eval_iters=200):\n",
    "    \"\"\"\n",
    "    Estimates the average loss on the training and validation sets.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The model to evaluate.\n",
    "    eval_iters (int): Number of evaluation iterations for averaging.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing mean loss for 'train' and 'val' sets.\n",
    "    \"\"\"\n",
    "    losses = {}  # Dictionary to store loss values\n",
    "    model.eval()  # Sets model to evaluation mode (important for layers like dropout)\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        split_losses = torch.zeros(eval_iters)  # Holds loss values for each iteration\n",
    "        for i in range(eval_iters):\n",
    "            # Get a batch of data for the current split ('train' or 'val')\n",
    "            x_batch, y_batch = get_batch(split, device=device)\n",
    "            # Perform a forward pass through the model to get predictions\n",
    "            logits = model(x_batch)\n",
    "            # Calculate loss between model predictions and actual values\n",
    "            loss = model.calculate_loss(logits, y_batch)\n",
    "            # Store the loss value\n",
    "            split_losses[i] = loss.item()\n",
    "        \n",
    "        # Calculate the mean loss for the current split\n",
    "        losses[split] = split_losses.mean()\n",
    "\n",
    "    model.train()  # Reset model to training mode\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "Now let’s proceed to set up the main training loop. This includes defining hyperparameters, setting the device, creating an optimiser, and iteratively adjusting the model weights based on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 0.21 million parameters\n",
      "Step 0000: Train Loss = 4.1622, Validation Loss = 4.1633\n",
      "Step 0100: Train Loss = 2.6702, Validation Loss = 2.6733\n",
      "Step 0200: Train Loss = 2.5020, Validation Loss = 2.5005\n",
      "Step 0300: Train Loss = 2.4623, Validation Loss = 2.4609\n",
      "Step 0400: Train Loss = 2.4038, Validation Loss = 2.3998\n",
      "Step 0500: Train Loss = 2.3359, Validation Loss = 2.3330\n",
      "Step 0600: Train Loss = 2.3081, Validation Loss = 2.3049\n",
      "Step 0700: Train Loss = 2.2813, Validation Loss = 2.3080\n",
      "Step 0800: Train Loss = 2.2317, Validation Loss = 2.2513\n",
      "Step 0900: Train Loss = 2.2236, Validation Loss = 2.2414\n",
      "Step 1000: Train Loss = 2.1879, Validation Loss = 2.2254\n",
      "Step 1100: Train Loss = 2.1890, Validation Loss = 2.2017\n",
      "Step 1200: Train Loss = 2.1839, Validation Loss = 2.2106\n",
      "Step 1300: Train Loss = 2.1571, Validation Loss = 2.1766\n",
      "Step 1400: Train Loss = 2.1315, Validation Loss = 2.1574\n",
      "Step 1500: Train Loss = 2.1298, Validation Loss = 2.1645\n",
      "Step 1600: Train Loss = 2.1149, Validation Loss = 2.1543\n",
      "Step 1700: Train Loss = 2.0948, Validation Loss = 2.1246\n",
      "Step 1800: Train Loss = 2.0930, Validation Loss = 2.1245\n",
      "Step 1900: Train Loss = 2.0747, Validation Loss = 2.1126\n",
      "Step 2000: Train Loss = 2.0830, Validation Loss = 2.1236\n",
      "Step 2100: Train Loss = 2.0663, Validation Loss = 2.1259\n",
      "Step 2200: Train Loss = 2.0432, Validation Loss = 2.0875\n",
      "Step 2300: Train Loss = 2.0203, Validation Loss = 2.0790\n",
      "Step 2400: Train Loss = 2.0133, Validation Loss = 2.0695\n",
      "Step 2500: Train Loss = 2.0531, Validation Loss = 2.0896\n",
      "Step 2600: Train Loss = 2.0054, Validation Loss = 2.0621\n",
      "Step 2700: Train Loss = 2.0133, Validation Loss = 2.0705\n",
      "Step 2800: Train Loss = 1.9907, Validation Loss = 2.0790\n",
      "Step 2900: Train Loss = 2.0025, Validation Loss = 2.0603\n",
      "Step 3000: Train Loss = 1.9779, Validation Loss = 2.0670\n",
      "Step 3100: Train Loss = 1.9832, Validation Loss = 2.0736\n",
      "Step 3200: Train Loss = 1.9668, Validation Loss = 2.0423\n",
      "Step 3300: Train Loss = 1.9508, Validation Loss = 2.0493\n",
      "Step 3400: Train Loss = 1.9756, Validation Loss = 2.0617\n",
      "Step 3500: Train Loss = 1.9516, Validation Loss = 2.0476\n",
      "Step 3600: Train Loss = 1.9452, Validation Loss = 2.0163\n",
      "Step 3700: Train Loss = 1.9485, Validation Loss = 2.0374\n",
      "Step 3800: Train Loss = 1.9499, Validation Loss = 2.0312\n",
      "Step 3900: Train Loss = 1.9122, Validation Loss = 2.0228\n",
      "Step 4000: Train Loss = 1.8988, Validation Loss = 2.0397\n",
      "Step 4100: Train Loss = 1.9105, Validation Loss = 2.0145\n",
      "Step 4200: Train Loss = 1.9201, Validation Loss = 2.0038\n",
      "Step 4300: Train Loss = 1.9242, Validation Loss = 2.0216\n",
      "Step 4400: Train Loss = 1.8937, Validation Loss = 2.0118\n",
      "Step 4500: Train Loss = 1.9095, Validation Loss = 2.0197\n",
      "Step 4600: Train Loss = 1.9087, Validation Loss = 2.0326\n",
      "Step 4700: Train Loss = 1.9043, Validation Loss = 2.0039\n",
      "Step 4800: Train Loss = 1.8758, Validation Loss = 2.0251\n",
      "Step 4900: Train Loss = 1.8800, Validation Loss = 1.9979\n",
      "Step 4999: Train Loss = 1.8826, Validation Loss = 2.0042\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters - these are parameters we set before training\n",
    "batch_size = 16          # Number of sequences processed in parallel\n",
    "context_length = 32      # Maximum context length the model considers\n",
    "max_steps = 5000         # Total number of optimisation steps\n",
    "eval_interval = 100      # Frequency of evaluation (in steps)\n",
    "learning_rate = 1e-3     # Step size for the optimiser\n",
    "\n",
    "# Determine whether a GPU is available, otherwise default to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the GPT language model and move it to the selected device\n",
    "gpt_net = GPTLanguageModel(vocab_size)\n",
    "gpt_net = gpt_net.to(device)\n",
    "\n",
    "# Display the number of parameters in the model (in millions) for reference\n",
    "print(f\"Model has {sum(p.numel() for p in gpt_net.parameters()) / 1e6:.2f} million parameters\")\n",
    "\n",
    "# Set up an optimiser, which updates model parameters to minimise loss\n",
    "optimizer = torch.optim.AdamW(gpt_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop - iterates over multiple steps to update model weights\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Evaluate the model at regular intervals on both train and validation sets\n",
    "    if step % eval_interval == 0 or step == max_steps - 1:\n",
    "        loss_values = estimate_loss(gpt_net)\n",
    "        print(f\"Step {step:04d}: Train Loss = {loss_values['train']:.4f}, Validation Loss = {loss_values['val']:.4f}\")\n",
    "    \n",
    "    # Fetch a batch of training data (input and target outputs)\n",
    "    x_batch, y_batch = get_batch('train', device=device)\n",
    "\n",
    "    # Forward pass: compute model predictions for the batch\n",
    "    logits = gpt_net(x_batch)\n",
    "    \n",
    "    # Calculate the training loss for the batch\n",
    "    loss = gpt_net.calculate_loss(logits, y_batch)\n",
    "\n",
    "    # Clear previous gradients to prepare for new backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Backward pass: compute gradients of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update model parameters using computed gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Training Steps\n",
    "\n",
    "1. **Hyperparameters**: We specify parameters like `batch_size`, `context_length`, `max_steps`, and `learning_rate` to control training behaviour. Adjusting these can impact how quickly and effectively the model learns.\n",
    "  \n",
    "2. **Device Selection**: Using a GPU (if available) allows the model to process data much faster than a CPU.\n",
    "  \n",
    "3. **Model Initialisation**: We create an instance of `GPTLanguageModel` and move it to the selected device.\n",
    "  \n",
    "4. **Optimiser Setup**: The optimiser, here `AdamW`, adjusts model parameters during training. `AdamW` is a variation of the popular Adam optimiser and helps control overfitting.\n",
    "\n",
    "5. **Main Training Loop**:\n",
    "   - **Evaluation**: Every `eval_interval` steps, the model’s performance is assessed using `estimate_loss`. This helps monitor learning progress and adjust parameters if necessary.\n",
    "   - **Batch Processing**: For each training step, we get a new batch of data using `get_batch`.\n",
    "   - **Forward Pass**: The model processes the batch, producing `logits` (predictions) for each input sequence.\n",
    "   - **Loss Calculation**: The `calculate_loss` function compares the model’s predictions to the true labels, giving a measure of how well the model is performing.\n",
    "   - **Backward Pass**: We compute gradients to see how to adjust model parameters.\n",
    "   - **Optimiser Step**: The optimiser updates the model’s weights based on the computed gradients, gradually reducing the loss.\n",
    "\n",
    "At the end of training, we observe that both the training and validation loss gradually decrease, reaching below 2 within 5000 steps. This steady reduction in loss suggests that our language model is learning the task effectively. For further improvement, you may increase the number of steps, which could reduce the loss even more and potentially improve the model's performance on complex or nuanced text. \n",
    "\n",
    "A lower loss indicates that the model is becoming more accurate at predicting the next word or token in a sequence, so continued training can refine this ability—though it’s essential to monitor the validation loss to avoid overfitting (when the model becomes too specialised on the training data at the expense of generalisation to new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "\n",
    "Now that our model is trained, let’s explore generating text. We can start with an empty context, represented by `torch.zeros((1, 1))`, which corresponds to a newline character (`\\n`). By setting `max_new_tokens=250`, we instruct the model to generate a sequence of 250 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whacence theirs no to not! makle\n",
      "This fit welse of or a brom wery repits. Were'er caross,\n",
      "I did that my leave Is for me, and would, me it vencess!\n",
      "\n",
      "AUDDY ARbawnly sweath pouch'd to dears, bendert my were can nows,\n",
      "The sun she so be high like you like\n"
     ]
    }
   ],
   "source": [
    "# Generate text from the model with an empty initial context\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = gpt_net.generate(context, max_new_tokens=250)[0].tolist()\n",
    "print(decode_txt(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide a custom context to see how the model continues a given text passage. Here, we supply a short poetic passage as a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O mighty mind, in circuits vast and deep,\n",
      "Thou dost all knowledge in thy logic keep.\n",
      "With language broad, and understanding high,\n",
      "Thou speaks as man, though but an artful lie.\n",
      "\n",
      "DUKE OF OMERCSAPLAMIBELLA:\n",
      "That for my which all sore-till'st that age so weptorbed have hums at be incciation, and you brannt my gnot have any love idserp mee even, the whey haves hour parittiz?\n",
      "\n",
      "TARD III:\n",
      "U, so wore but me, would which you ran the\n"
     ]
    }
   ],
   "source": [
    "# Define a custom context for text generation\n",
    "context = \"\"\"\n",
    "O mighty mind, in circuits vast and deep,\n",
    "Thou dost all knowledge in thy logic keep.\n",
    "With language broad, and understanding high,\n",
    "Thou speaks as man, though but an artful lie.\n",
    "\"\"\"\n",
    "\n",
    "# Encode the custom context and reshape it to match input format\n",
    "context = torch.tensor(encode_txt(context))\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "context = context.to(device)\n",
    "\n",
    "# Generate text based on the provided context\n",
    "generated_text = gpt_net.generate(context, max_new_tokens=250)[0].tolist()\n",
    "print(decode_txt(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "## Discussion\n",
    "\n",
    "In this notebook, we built a simple language model that mimics, at a basic level, the behaviour of large language models (LLMs) like ChatGPT and Gemini. Although our model is simpler, it follows similar principles, using transformers and self-attention mechanisms to generate text based on learned data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are some exercises to further explore and enhance your understanding:\n",
    "\n",
    "1. **Experiment with different prompts**: Try providing various types of starting contexts and see how the model responds. Does it produce coherent and contextually relevant text?\n",
    "   \n",
    "2. **Adjust `max_new_tokens`**: Observe how changing the length of generated text impacts the model’s output quality and coherence.\n",
    "\n",
    "3. **Increase model capacity**: Experiment with changing the model architecture, such as adding more transformer layers or heads. Monitor how these changes impact training time and the quality of generated text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
